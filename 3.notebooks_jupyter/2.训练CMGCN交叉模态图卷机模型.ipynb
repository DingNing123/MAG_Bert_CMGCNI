{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49af67e-5ef9-477a-9c77-08bdf46e6377",
   "metadata": {},
   "source": [
    "# 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abd8a00-e4c7-490c-9c26-8076e69c0d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_feature': (33, 33),\n",
       " 'video_features_p': (10, 768),\n",
       " 'bert_indices': (24,),\n",
       " 'box_pad_indices': (10, 3),\n",
       " 'big_graphs': (34, 34),\n",
       " 'labels': numpy.int64}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "class MUStartDataset(Dataset):\n",
    "    def __init__(self,mode = 'train',feature_path = './featuresIndepResnet152.pkl'):\n",
    "        with open(feature_path,'rb') as f:\n",
    "            import pickle\n",
    "            data = pickle.load(f)\n",
    "        self.feature_dict = data[mode]\n",
    "        # print(self.feature_dict['labels'][:10])\n",
    "        # [-1,1] -> [0,1]\n",
    "        self.feature_dict['labels'] = ((self.feature_dict['labels'] + 1)/2).astype(np.int64)\n",
    "        # print(self.feature_dict['labels'][:10])\n",
    "        # self.feature_dict['labels'] = np.expand_dims(self.feature_dict['labels'], axis=-1)\n",
    "        # print('init ', mode, 'dataset ', self.feature_dict.keys())\n",
    "    def __getitem__(self,index):\n",
    "        feature ={}\n",
    "        feature['audio_feature'] = self.feature_dict['audio_feature'][index]\n",
    "        feature['video_features_p'] = self.feature_dict['video_features_p'][index]\n",
    "        feature['bert_indices'] = self.feature_dict['bert_indices'][index]\n",
    "        feature['box_pad_indices'] = self.feature_dict['box_pad_indices'][index]\n",
    "        feature['big_graphs'] = self.feature_dict['big_graphs'][index]\n",
    "        feature['labels'] = self.feature_dict['labels'][index]\n",
    "        \n",
    "        return feature\n",
    "    def __len__(self):\n",
    "        labels = self.feature_dict['labels']\n",
    "        length = labels.shape[0]\n",
    "        return length\n",
    "    \n",
    "    def get_sample_shape(self,index):\n",
    "        shape_dict = {}\n",
    "        shape_dict['audio_feature'] = self.feature_dict['audio_feature'][index].shape\n",
    "        shape_dict['video_features_p'] = self.feature_dict['video_features_p'][index].shape\n",
    "        shape_dict['bert_indices'] = self.feature_dict['bert_indices'][index].shape\n",
    "        shape_dict['box_pad_indices'] = self.feature_dict['box_pad_indices'][index].shape\n",
    "        shape_dict['big_graphs'] = self.feature_dict['big_graphs'][index].shape\n",
    "        # shape_dict['labels'] = self.feature_dict['labels'][index].shape\n",
    "        shape_dict['labels'] = type(self.feature_dict['labels'][index])\n",
    "        return shape_dict\n",
    "        \n",
    "d = MUStartDataset('valid')\n",
    "d.get_sample_shape(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e5bf18-87a2-4244-b691-e0195ffa1c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio_feature', 'video_features_p', 'bert_indices', 'box_pad_indices', 'big_graphs', 'labels'])\n",
      "audio_feature torch.Size([2, 33, 33])\n",
      "video_features_p torch.Size([2, 10, 768])\n",
      "bert_indices torch.Size([2, 24])\n",
      "box_pad_indices torch.Size([2, 10, 3])\n",
      "big_graphs torch.Size([2, 34, 34])\n",
      "labels torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(d, batch_size=2, num_workers=0, shuffle=False)\n",
    "for batch in dl:\n",
    "    print(batch.keys())\n",
    "    for key in batch.keys():\n",
    "        print(key, batch[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7c959c-a4ee-48f0-b9cf-a2e1c1e67831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda\n",
    "device = torch.device('cuda:0') if use_cuda else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dada7628-9db6-459b-adbc-cf1492544d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create CMGCN model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-uncased/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias :\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias',None)\n",
    "        \n",
    "    def forward(self, text, adj):\n",
    "        hidden = torch.matmul(text,self.weight)\n",
    "        \n",
    "        denom = torch.sum(adj,dim=2,keepdim=True) + 1\n",
    "        output = torch.matmul(adj, hidden.float())/denom\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "\n",
    "        return output\n",
    "\n",
    "class CMGCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CMGCN, self).__init__()\n",
    "        print('create CMGCN model')\n",
    "        self.bert = BertModel.from_pretrained('./bert-base-uncased/')\n",
    "        self.text_lstm = DynamicLSTM(768,4,num_layers=1,batch_first=True,bidirectional=True)\n",
    "        self.vit_fc = nn.Linear(768,2*4)\n",
    "        self.gc1 = GraphConvolution(2*4, 2*4)\n",
    "        self.gc2 = GraphConvolution(2*4, 2*4)\n",
    "        self.fc = nn.Linear(2*4,2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bert_indices = inputs['bert_indices']\n",
    "        graph = inputs['big_graphs']\n",
    "        box_vit = inputs['video_features_p']\n",
    "        bert_text_len = torch.sum(bert_indices != 0, dim = -1)\n",
    "        outputs = self.bert(bert_indices)\n",
    "        encoder_layer = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        text_out, (_, _) = self.text_lstm(encoder_layer, bert_text_len)\n",
    "        # 与原始代码不同，这里因为进行了全局的特征填充，导致text_out可能无法达到填充长度，补充为0\n",
    "        if text_out.shape[1] < encoder_layer.shape[1]:\n",
    "            pad = torch.zeros((text_out.shape[0],encoder_layer.shape[1]-text_out.shape[1],text_out.shape[2]))\n",
    "            text_out = torch.cat((text_out,pad),dim=1)\n",
    "\n",
    "        box_vit = box_vit.float()\n",
    "        box_vit = self.vit_fc(box_vit)\n",
    "        features = torch.cat([text_out, box_vit], dim=1)\n",
    "\n",
    "        graph = graph.float()\n",
    "        x = F.relu(self.gc1(features, graph))\n",
    "        x = F.relu(self.gc2(x,graph))\n",
    "        \n",
    "        alpha_mat = torch.matmul(features,x.transpose(1,2))\n",
    "        alpha_mat = alpha_mat.sum(1, keepdim=True)\n",
    "        alpha = F.softmax(alpha_mat, dim = 2)\n",
    "        x = torch.matmul(alpha, x).squeeze(1)\n",
    "        \n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "cmgcn_model = CMGCN().to(device)\n",
    "len(list(cmgcn_model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac702a4f-b713-42cb-9512-19203855a0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cmgcn_model.bert.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6b9c70-eace-44e1-bddb-42ec33cf1d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':cmgcn_model.bert.parameters(),'lr':2e-5},\n",
    "    {'params':cmgcn_model.text_lstm.parameters(),},\n",
    "    {'params':cmgcn_model.vit_fc.parameters(),},\n",
    "    {'params':cmgcn_model.gc1.parameters(),},\n",
    "    {'params':cmgcn_model.gc2.parameters(),},\n",
    "    {'params':cmgcn_model.fc.parameters(),},\n",
    "],lr=0.001,weight_decay=1e-5)\n",
    "# optimizer = torch.optim.Adam(cmgcn_model.parameters(),lr=1e-3,weight_decay=1e-5)\n",
    "# optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a69f4-097e-4c5f-9680-40a9b4a29805",
   "metadata": {},
   "source": [
    "# 初始化训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7012e3-c2cb-4ccd-aca7-1aeb6417cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_params()\n"
     ]
    }
   ],
   "source": [
    "def init_params():\n",
    "    for child in cmgcn_model.children():\n",
    "        # print(type(child) != BertModel)\n",
    "        if type(child) != BertModel:\n",
    "            for p in child.parameters():\n",
    "                # print(type(child))\n",
    "                # print(p.shape, p.requires_grad)\n",
    "                if p.requires_grad :\n",
    "                    # print(len(p.shape))\n",
    "                    if len(p.shape) > 1:\n",
    "                        torch.nn.init.xavier_uniform_(p)\n",
    "                        # print(p[0][:2])\n",
    "                    else:\n",
    "                        import math\n",
    "                        stdv = 1.0 / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "                        # print('else', p[:2])\n",
    "    print('init_params()')\n",
    "                    \n",
    "init_params()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05baca10-ac52-4c71-acd7-a87179aa0630",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0de29f4a-50c5-4960-84cb-c23e3eb08bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train:----------\n",
      "i_epoch: 0\n",
      "here save the model cmgcn_model.pth\n",
      "early stop\n",
      "test_acc: 0.5\n",
      "test_f1: 0.3333333333333333\n",
      "test_precision 0.25\n",
      "test_recall 0.5\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 1\n",
    "cmgcn_model_path = 'cmgcn_model.pth'\n",
    "\n",
    "# def train():\n",
    "print('start train:' + '-'*10)\n",
    "train_dataset = MUStartDataset(mode='train')\n",
    "valid_dataset = MUStartDataset(mode='valid')\n",
    "test_dataset = MUStartDataset(mode='test')\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=2,num_workers=0,shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=2,num_workers=0,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=2,num_workers=0,shuffle=False)\n",
    "\n",
    "def evaluate_acc_f1(data_loader):\n",
    "    n_correct, n_total = 0, 0\n",
    "    targets_all, outputs_all = None, None\n",
    "    cmgcn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i_batch,batch in enumerate(data_loader):\n",
    "            inputs ={}\n",
    "            for key in batch.keys():\n",
    "                inputs[key] = batch[key].to(device)\n",
    "            outputs = cmgcn_model(inputs)\n",
    "            targets = batch['labels'].to(device)\n",
    "            \n",
    "            n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "            n_total += len(outputs)\n",
    "            \n",
    "            if targets_all is None:\n",
    "                targets_all = targets\n",
    "                outputs_all = outputs\n",
    "            else:\n",
    "                targets_all = torch.cat((targets_all,targets), dim=0)\n",
    "                outputs_all = torch.cat((outputs_all,outputs), dim=0)\n",
    "    \n",
    "    # if macro :\n",
    "    from sklearn import metrics\n",
    "    acc = n_correct / n_total\n",
    "    f1 = metrics.f1_score(targets_all.cpu(), torch.argmax(outputs_all,-1).cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "    precision = metrics.precision_score(targets_all.cpu(), torch.argmax(outputs_all,-1).cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "    recall = metrics.recall_score(targets_all.cpu(), torch.argmax(outputs_all,-1).cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "\n",
    "    return acc,f1,precision,recall\n",
    "\n",
    "max_val_acc , max_val_f1, max_val_epoch, global_step = 0, 0, 0, 0\n",
    "for i_epoch in range(num_epoch):\n",
    "    print('i_epoch:', i_epoch)\n",
    "    n_correct, n_total, loss_total = 0, 0, 0\n",
    "    for i_batch,batch in enumerate(train_dataloader):\n",
    "        global_step += 1\n",
    "        cmgcn_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        inputs ={}\n",
    "        for key in batch.keys():\n",
    "            inputs[key] = batch[key].to(device)\n",
    "        # print(inputs.keys()) \n",
    "        # dict_keys(['audio_feature', 'video_features_p', 'bert_indices', 'box_pad_indices', 'big_graphs', 'labels'])\n",
    "        outputs = cmgcn_model(inputs)\n",
    "        targets = batch['labels'].to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "        n_total += len(outputs)\n",
    "        loss_total += loss.item() * len(outputs)\n",
    "        \n",
    "        train_acc = n_correct / n_total\n",
    "        train_loss = loss_total / n_total\n",
    "        \n",
    "        if global_step % 1 == 0:\n",
    "            val_acc, val_f1, val_precision, val_recall = evaluate_acc_f1(valid_dataloader)\n",
    "            if val_acc >= max_val_acc:\n",
    "                max_val_f1 = val_f1\n",
    "                max_val_acc = val_acc\n",
    "                max_val_epoch = i_epoch\n",
    "                torch.save(cmgcn_model.state_dict(),cmgcn_model_path)\n",
    "                print('here save the model cmgcn_model.pth')\n",
    "        \n",
    "    if i_epoch - max_val_epoch >= 0:\n",
    "        print('early stop')\n",
    "        break\n",
    "        \n",
    "    break\n",
    "cmgcn_model.load_state_dict(torch.load(cmgcn_model_path))\n",
    "test_acc, test_f1,test_precision,test_recall = evaluate_acc_f1(test_dataloader)\n",
    "# test_acc, test_f1,test_precision,test_recall = evaluate_acc_f1(test_data_loader)\n",
    "print('test_acc:', test_acc)\n",
    "print('test_f1:', test_f1)\n",
    "print('test_precision', test_precision)\n",
    "print('test_recall', test_recall)\n",
    "\n",
    "# return 0\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c82b239-715b-485a-bc1a-9909763c2f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18716.53s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 mac staff 418M 10 24 18:10 cmgcn_model.pth\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {cmgcn_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d00678-f62c-4a62-8177-4a26d283b0d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 调试形状"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c52b6e66-ff14-4dfb-9b0e-ddb0290692dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "bert = BertModel.from_pretrained('./bert-base-uncased/').to(device)\n",
    "train_dataset = MUStartDataset(mode='train')\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=2,num_workers=0,shuffle=False)\n",
    "batch = iter(train_dataloader).next()\n",
    "# print(batch.keys())\n",
    "# for i_batch,batch in enumerate(train_dataloader):\n",
    "cmgcn_model.train()\n",
    "optimizer.zero_grad()\n",
    "inputs ={}\n",
    "for key in batch.keys():\n",
    "    inputs[key] = batch[key].to(device)\n",
    "\n",
    "# print(inputs.keys()) \n",
    "# dict_keys(['audio_feature', 'video_features_p', 'bert_indices', 'box_pad_indices', 'big_graphs', 'labels'])\n",
    "bert_indices = inputs['bert_indices']\n",
    "graph = inputs['big_graphs']\n",
    "box_vit = inputs['video_features_p']\n",
    "print('bert_indices,graph,box_vit ',bert_indices.shape, graph.shape, box_vit.shape)\n",
    "bert_text_len = torch.sum(bert_indices != 0, dim = -1)\n",
    "print('bert_text_len', bert_text_len)\n",
    "outputs = bert(bert_indices)\n",
    "encoder_layer = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output\n",
    "print('encoder_layer', encoder_layer.shape)\n",
    "print('pooled_output:', pooled_output.shape)\n",
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "text_lstm = DynamicLSTM(768,4,num_layers=1,batch_first=True,bidirectional=True)\n",
    "\n",
    "text_out, (_, _) = text_lstm(encoder_layer, bert_text_len)\n",
    "print('text_out:', text_out.shape)\n",
    "\n",
    "box_vit = box_vit.float()\n",
    "vit_fc = nn.Linear(768,2*4)\n",
    "box_vit = vit_fc(box_vit)\n",
    "print('box_vit:', box_vit.shape)\n",
    "features = torch.cat([text_out, box_vit], dim=1)\n",
    "print('feature', features.shape)\n",
    "\n",
    "graph = graph.float()\n",
    "# x = F.relu(gc1(features, graph))\n",
    "text = features\n",
    "adj = graph\n",
    "weight = nn.Parameter(torch.FloatTensor(2*4,2*4))\n",
    "# print('text',text)\n",
    "# print('weight',weight)\n",
    "hidden = torch.matmul(text,weight)\n",
    "print('hidden:', hidden.shape)\n",
    "denom = torch.sum(adj,dim=2,keepdim=True) + 1\n",
    "# print('denom:',denom.shape, denom)\n",
    "output = torch.matmul(adj, hidden.float())/denom\n",
    "print('output:', output.shape)\n",
    "\n",
    "x = output\n",
    "# print('x', x.shape, x)\n",
    "alpha_mat = torch.matmul(features,x.transpose(1,2))\n",
    "print('alpha_mat:', alpha_mat.shape)\n",
    "alpha_mat = alpha_mat.sum(1, keepdim=True)\n",
    "print('alpha_mat_sum:',alpha_mat.shape)\n",
    "import torch.nn.functional as F\n",
    "alpha = F.softmax(alpha_mat, dim = 2)\n",
    "print('alpha', alpha.shape)\n",
    "x = torch.matmul(alpha, x).squeeze(1)\n",
    "# print('x', x.shape, x)\n",
    "fc = nn.Linear(2*4,2)\n",
    "output = fc(x)\n",
    "print('output', output.shape)\n",
    "    # outputs = cmgcn_model(inputs)\n",
    "\n",
    "targets = batch['labels'].to(device)\n",
    "print('targets', targets.shape)\n",
    "outputs = output\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "targets = targets.long()\n",
    "print('targets.dtype', targets.dtype, 'targets', targets)\n",
    "print('outputs',outputs)\n",
    "loss = criterion(outputs, targets)\n",
    "print('loss',loss, type(loss))\n",
    "# 这时需要初始化参数 否则容易nan\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e78d8b5e-91be-4d59-a477-e1505b908a2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"./bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d23c4d5-4b59-475d-bf12-bb4448171b8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "i = 0\n",
    "for x,y in cmgcn_model.named_parameters():\n",
    "    print(x,y.shape)\n",
    "    i += 1\n",
    "    if i==10:\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
