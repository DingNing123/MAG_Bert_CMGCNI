{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9403c44a-38c1-495f-bca1-a28e1be92b3f",
   "metadata": {},
   "source": [
    "macOS 出现了找不到libffi.7这个库\n",
    "brew install 之后 \n",
    "建立符号链接\n",
    "LD_LIBRARY_PATH 使用这个环境变量添加查找共享库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fb719-4457-4819-8eea-4735f1383c45",
   "metadata": {},
   "source": [
    "# 生成speaker_independent的划分的索引文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b0573d-f6ad-4b4a-aeb9-8acf815d8f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 554 136\n",
      "2 2\n",
      "['1_70', '1_276'] ['1_60', '1_80']\n",
      "[1, 9] [0, 2]\n"
     ]
    }
   ],
   "source": [
    "# debug : 只测试两条数据以完善代码。\n",
    "import json\n",
    "from loggers import get_stderr_file_logger\n",
    "\n",
    "log_file = '11.log'\n",
    "logger = get_stderr_file_logger(log_file)\n",
    "logging.info(\"This is an INFO message\")\n",
    "logging.warning(\"This is a WARNING message\")\n",
    "\n",
    "\n",
    "debug = True\n",
    "json_file = 'sarcasm_data.json'\n",
    "\n",
    "with open(json_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "train_index = []\n",
    "test_index = []\n",
    "for id,ID in enumerate(list(data.keys())):\n",
    "    speaker = data[ID]['speaker']\n",
    "    if speaker == 'HOWARD' or speaker == 'SHELDON':\n",
    "        test_index.append(id)\n",
    "        test_ids.append(ID)\n",
    "    else:\n",
    "        train_index.append(id)\n",
    "        train_ids.append(ID)\n",
    "        \n",
    "print('total:', len(train_ids), len(test_ids))\n",
    "\n",
    "if debug:\n",
    "    DATA_PIECES = 2\n",
    "    train_index = train_index[:DATA_PIECES]\n",
    "    test_index = test_index[:DATA_PIECES]\n",
    "    train_ids = train_ids[:DATA_PIECES]\n",
    "    test_ids = test_ids[:DATA_PIECES]\n",
    "    \n",
    "print(len(train_ids), len(test_ids))\n",
    "print(train_ids[:DATA_PIECES], test_ids[:DATA_PIECES])\n",
    "print(train_index[:DATA_PIECES], test_index[:DATA_PIECES])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8e312-ffe9-4d63-82dd-d7a0d384e048",
   "metadata": {},
   "source": [
    "# 生成需要的标签文件 label_indep.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0824d89f-c305-4f4a-8485-2528a3708deb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1_70 train\n",
      "{'video_id': '1_70', 'clip_id': 0, 'text': 'i don t think i ll be able to stop thinking about it', 'label': 1.0, 'annotation': 'Positive', 'mode': 'train', 'label_by': 0}\n",
      "9 1_276 train\n",
      "{'video_id': '1_276', 'clip_id': 0, 'text': 'yeah my parents felt that naming me leonard and putting me in advanced placement classes wasn t getting me beaten up enough', 'label': 1.0, 'annotation': 'Positive', 'mode': 'train', 'label_by': 0}\n",
      "0 1_60 valid\n",
      "{'video_id': '1_60', 'clip_id': 0, 'text': 'it s just a privilege to watch your mind at work', 'label': 1.0, 'annotation': 'Positive', 'mode': 'valid', 'label_by': 0}\n",
      "2 1_80 valid\n",
      "{'video_id': '1_80', 'clip_id': 0, 'text': 'since it s not bee season you can have my epinephrine', 'label': -1.0, 'annotation': 'Negative', 'mode': 'valid', 'label_by': 0}\n",
      "0 1_60 test\n",
      "{'video_id': '1_60', 'clip_id': 0, 'text': 'it s just a privilege to watch your mind at work', 'label': 1.0, 'annotation': 'Positive', 'mode': 'test', 'label_by': 0}\n",
      "2 1_80 test\n",
      "{'video_id': '1_80', 'clip_id': 0, 'text': 'since it s not bee season you can have my epinephrine', 'label': -1.0, 'annotation': 'Negative', 'mode': 'test', 'label_by': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils.functions import csv_header, csv_add_one_row, get_file_size\n",
    "\n",
    "def clean_text_remove_punctuation(text):\n",
    "    '''\n",
    "    清理文本中的标点符号,并且转换为小写，合并连续的空格为一个\n",
    "    '''\n",
    "    punctuation = '!,;:?\"、，；.'\n",
    "    import re\n",
    "    text1 = re.sub(r'[{}]+'.format(punctuation),' ',text)\n",
    "    text2 = re.sub(r'[\\']',' ',text1)\n",
    "    text2 = text2.strip().lower()\n",
    "    text2 = ' '.join(text2.split())\n",
    "    return text2\n",
    "\n",
    "def add_split(mode_index, mode ='train'):\n",
    "    for idx, ID in enumerate(list(data.keys())):\n",
    "            video_id = ID\n",
    "            clip_id = 0 \n",
    "            text = data[ID]['utterance']\n",
    "            text = clean_text_remove_punctuation(text)\n",
    "            \n",
    "            label = 1.0 if data[ID]['sarcasm'] else -1.0\n",
    "            annotation = 'Positive' if data[ID]['sarcasm'] else 'Negative'        # train valid test\n",
    "            label_by = 0 \n",
    "            if idx in mode_index:\n",
    "                print(idx, video_id, mode)\n",
    "                row = {'video_id':video_id,\n",
    "                        'clip_id':clip_id,\n",
    "                        'text':text,\n",
    "                        'label':label,\n",
    "                        'annotation':annotation,\n",
    "                        'mode':mode,\n",
    "                        'label_by':label_by,\n",
    "                        }\n",
    "                print(row)\n",
    "                csv_add_one_row(label_csv, fieldnames, row)\n",
    "\n",
    "label_csv = 'label_indep.csv'\n",
    "fieldnames = ['video_id', 'clip_id','text', 'label', \n",
    "              'annotation','mode', 'label_by']\n",
    "\n",
    "csv_header(label_csv, fieldnames)\n",
    "add_split(train_index, mode = 'train')\n",
    "add_split(test_index, mode = 'valid')\n",
    "add_split(test_index, mode = 'test')    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7495c007-77c7-4829-aa9b-517744bcff70",
   "metadata": {
    "tags": []
   },
   "source": [
    "!cat -n  label_indep.csv | head\n",
    "!wc {label_csv}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6683896-6452-4bcc-a7e4-ed26f98a4e7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 视频拆分为帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8890780f-705c-4253-b0f0-b32a2e3cac5f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmsd_raw_data/utterances_final\\1_10004.mp4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_348\\2880464586.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvideos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mvideo_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvideo_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_ids\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mdirName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvideo_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "video_path = 'mmsd_raw_data/utterances_final/'\n",
    "frame_path = 'mmsd_raw_data/Processed/video/Frames/'\n",
    "\n",
    "videos = glob(video_path+'*')\n",
    "\n",
    "for video in videos:\n",
    "    print(video)\n",
    "    video_id = video.split('/')[2].split('.')[0]\n",
    "    if video_id in train_ids + test_ids:\n",
    "        dirName = frame_path + video_id\n",
    "        if not os.path.exists(dirName):\n",
    "            os.mkdir(dirName)\n",
    "            print(\"Directory \" , dirName ,  \" Created \")\n",
    "            input_mp4 = video_path + video_id + \".mp4\"\n",
    "            ffmpeg = '/Users/mac/anaconda3/envs/t18/bin/ffmpeg'\n",
    "            cmd = \"{} -i {} -vf fps=1 {}/%5d.jpg\".format(ffmpeg,input_mp4,dirName)\n",
    "            print(cmd)\n",
    "            os.system(cmd)\n",
    "        else:\n",
    "            print(\"Directory \" , dirName ,  \" already exists\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c5baa-9233-4b41-a5e6-f9d54927dc8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tree {frame_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cdf46b-0adc-4506-8dd1-b56158659922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(frame_path + '1_90/00001.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa96f1-57de-4fa7-b767-28e5cb7cf107",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep '1_90' {label_csv}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b1a65-1d98-4cf9-9cf2-419794dc4ed8",
   "metadata": {},
   "source": [
    "# 提取特征"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb33b4b6-9f15-4c5b-ac6c-ab2406ed29fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "!tree mmsd_raw_data/Processed/audio\n",
    "!mv mmsd_raw_data/Processed/audio/1_70/ /tmp/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe92d6b-e654-4294-b552-3c24453b21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultTrainer, default_setup, launch\n",
    "from detectron2.evaluation import COCOEvaluator, verify_results\n",
    "from bua.caffe import add_bottom_up_attention_config\n",
    "import cv2\n",
    "from extract_utils import get_image_blob\n",
    "from bua.caffe.modeling.layers.nms import nms\n",
    "import torch\n",
    "\n",
    "output_path = './featuresIndepResnet152.pkl'\n",
    "audios_path = 'mmsd_raw_data/Processed/audio/'\n",
    "data_path = 'evaluation'\n",
    "\n",
    "df = pd.read_csv(label_csv)\n",
    "# Load classes\n",
    "classes = ['__background__']\n",
    "with open(os.path.join(data_path, 'objects_vocab.txt')) as f:\n",
    "    for object in f.readlines():\n",
    "        classes.append(object.split(',')[0].lower().strip())\n",
    "print(len(classes))\n",
    "print(classes[:10])\n",
    "\n",
    "# Load attributes\n",
    "attributes = ['__no_attribute__']\n",
    "with open(os.path.join(data_path, 'attributes_vocab.txt')) as f:\n",
    "    for att in f.readlines():\n",
    "        attributes.append(att.split(',')[0].lower().strip())\n",
    "print(len(attributes))\n",
    "print(attributes[:10])\n",
    "\n",
    "\n",
    "config_file = 'configs/caffe/test-caffe-r152.yaml'\n",
    "cfg = get_cfg()\n",
    "cfg.MODEL.DEVICE = 'cpu'\n",
    "\n",
    "add_bottom_up_attention_config(cfg, True)\n",
    "cfg.merge_from_file(config_file)\n",
    "cfg.merge_from_list(['MODEL.BUA.EXTRACT_FEATS',True])\n",
    "cfg.freeze()\n",
    "\n",
    "\n",
    "model = DefaultTrainer.build_model(cfg)\n",
    "DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
    "    cfg.MODEL.WEIGHTS, resume=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "MIN_BOXES = 10\n",
    "MAX_BOXES = 10\n",
    "CONF_THRESH = 0.4\n",
    "\n",
    "def model_inference(model, batched_inputs, mode):\n",
    "    if mode == \"caffe\":\n",
    "        return model(batched_inputs)\n",
    "    elif mode == \"d2\":\n",
    "        images = model.preprocess_image(batched_inputs)\n",
    "        features = model.backbone(images.tensor)\n",
    "    \n",
    "        if model.proposal_generator:\n",
    "            proposals, _ = model.proposal_generator(images, features, None)\n",
    "        else:\n",
    "            assert \"proposals\" in batched_inputs[0]\n",
    "            proposals = [x[\"proposals\"].to(model.device) for x in batched_inputs]\n",
    "\n",
    "        return model.roi_heads(images, features, proposals, None)\n",
    "    else:\n",
    "        raise Exception(\"detection model not supported: {}\".format(mode))\n",
    "        \n",
    "\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def video_to_wav(video_file, audio_file):\n",
    "    cmd = 'ffmpeg -v quiet  -i ' + video_file + ' -f wav -vn ' + audio_file\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "def audio_embedding(audio_file):\n",
    "    audio_feature = None\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    hop_length = 512 * 8 \n",
    "    # hop_length smaller, seq_len larger\n",
    "    f0 = librosa.feature.zero_crossing_rate(y, hop_length = hop_length).T\n",
    "    mfcc = librosa.feature.mfcc(y=y,sr=sr,hop_length=hop_length).T\n",
    "    cqt = librosa.feature.chroma_cqt(y=y,sr=sr,hop_length=hop_length).T\n",
    "    audio_feature = np.concatenate([f0, mfcc, cqt], axis = -1)\n",
    "    # print(audio_feature.shape)\n",
    "    return audio_feature\n",
    "\n",
    "def padding_sequence(sequences):\n",
    "    '''\n",
    "    return 填充后的形状统一的特征,截断后的序列长度数组\n",
    "    原始数据集长度差异太大了。\n",
    "    '''\n",
    "    features = None\n",
    "    feature_dim = sequences[0].shape[-1]\n",
    "    lens = [s.shape[0] for s in sequences]\n",
    "    final_length = int(np.mean(lens) + 1 * np.std(lens))\n",
    "    features = np.zeros([len(sequences), final_length, feature_dim])\n",
    "    sequence_lenth_array = []\n",
    "    for i, s in enumerate(sequences):\n",
    "        # features[i] = s + [0000]\n",
    "        # feature = s\n",
    "        # MAX_LEN = final_length \n",
    "        # 为了避免后期LSTM长度错误，记录截断后的长度\n",
    "        length = s.shape[0]\n",
    "        if length >= final_length:\n",
    "            features[i] = s[:final_length, :]\n",
    "            length = final_length\n",
    "            # print('截断')\n",
    "        else:\n",
    "            pad = np.zeros([final_length - length, feature_dim])\n",
    "            features[i] = np.concatenate((s, pad), axis = 0)\n",
    "            # print('pad end', pad.shape)\n",
    "        sequence_lenth_array.append(length)\n",
    "    \n",
    "    return features, sequence_lenth_array\n",
    "    \n",
    "def file_2_10_boxes_list(im_file):    \n",
    "    im = cv2.imread(im_file)\n",
    "    dataset_dict = get_image_blob(im, cfg.MODEL.PIXEL_MEAN)\n",
    "    \n",
    "    mode = \"caffe\"\n",
    "    img_id = im_file\n",
    "    with torch.set_grad_enabled(False):\n",
    "        boxes, scores, features_pooled, attr_scores = model_inference(model,[dataset_dict],mode)\n",
    "    \n",
    "    dets = boxes[0].tensor.cpu() / dataset_dict['im_scale']\n",
    "    scores = scores[0].cpu()\n",
    "    feats = features_pooled[0].cpu()\n",
    "    attr_scores = attr_scores[0].cpu()\n",
    "    \n",
    "    max_conf = torch.zeros((scores.shape[0])).to(scores.device)\n",
    "    for cls_ind in range(1, scores.shape[1]):\n",
    "            cls_scores = scores[:, cls_ind]\n",
    "            keep = nms(dets, cls_scores, 0.3)\n",
    "            max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],\n",
    "                                        cls_scores[keep],\n",
    "                                        max_conf[keep])\n",
    "\n",
    "    keep_boxes = torch.nonzero(max_conf >= CONF_THRESH).flatten()\n",
    "    if len(keep_boxes) < MIN_BOXES:\n",
    "        keep_boxes = torch.argsort(max_conf, descending=True)[:MIN_BOXES]\n",
    "    elif len(keep_boxes) > MAX_BOXES:\n",
    "        keep_boxes = torch.argsort(max_conf, descending=True)[:MAX_BOXES]\n",
    "\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    boxes = dets[keep_boxes].numpy()\n",
    "    import numpy as np\n",
    "\n",
    "    objects = np.argmax(scores[keep_boxes].numpy()[:,1:], axis=1)\n",
    "    attr_thresh = 0.1\n",
    "    attr = np.argmax(attr_scores[keep_boxes].numpy()[:,1:], axis=1)\n",
    "    attr_conf = np.max(attr_scores[keep_boxes].numpy()[:,1:], axis=1)\n",
    "    \n",
    "    box_list = list()\n",
    "    \n",
    "    for i in range(len(keep_boxes)):\n",
    "        bbox = boxes[i]\n",
    "        if bbox[0] == 0:\n",
    "            bbox[0] = 1\n",
    "        if bbox[1] == 0:\n",
    "            bbox[1] = 1\n",
    "\n",
    "        if mode == \"caffe\":\n",
    "            cls = classes[objects[i]+1]  # caffe +2\n",
    "            if attr_conf[i] > attr_thresh:\n",
    "                cls = attributes[attr[i]+1] + \" \" + cls   #  caffe +2\n",
    "        elif mode == \"d2\":\n",
    "            cls = classes[objects[i]+2]  # d2 +2\n",
    "            if attr_conf[i] > attr_thresh:\n",
    "                cls = attributes[attr[i]+2] + \" \" + cls   # d2 +2\n",
    "        else:\n",
    "            raise Exception(\"detection model not supported: {}\".format(mode))\n",
    "\n",
    "        # print('x1,y1,x2,y2 : ', bbox,'  cls:', cls)\n",
    "        box_list.append([bbox,cls])\n",
    "    \n",
    "    return box_list\n",
    "\n",
    "def generate_10_box_pics(video_id, frame0, box_list):\n",
    "    box_path = './mmsd_raw_data/Processed/video/box/'\n",
    "\n",
    "    im_dir = box_path + video_id\n",
    "    import os\n",
    "    isExist = os.path.exists(im_dir)\n",
    "    if not isExist:\n",
    "        os.makedirs(im_dir)\n",
    "        print(\"The new directory {} is created!\".format(im_dir))\n",
    "    img_path = frame0\n",
    "    im = cv2.imread(img_path)\n",
    "    # im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    lines = []\n",
    "    for idx, box in enumerate(box_list):\n",
    "        box , cls_name = box\n",
    "        x1,y1,x2,y2 = box\n",
    "        x1,y1,x2,y2 = int(x1),int(y1),int(x2),int(y2)\n",
    "        patch_img = im[y1:y2,x1:x2]\n",
    "        patch_name = im_dir +'/'+str(idx)+'.jpg'\n",
    "        cv2.imwrite(patch_name,patch_img)\n",
    "        lines.append(str(idx) + ' ' + cls_name + '\\n')\n",
    "    with open(im_dir+'.txt','w') as f:\n",
    "        f.writelines(lines)\n",
    "    # print(len(lines), im_dir + '.txt')\n",
    "\n",
    "\n",
    "\n",
    "def vit_feature(video_id):\n",
    "    box_path = './mmsd_raw_data/Processed/video/box/'\n",
    "    \n",
    "    txt = box_path + video_id + '.txt'\n",
    "    im_id = video_id\n",
    "    with open(txt) as f:\n",
    "        lines = f.readlines()\n",
    "    # 为了能批量处理特征所以生成图片列表 \n",
    "    features = []\n",
    "    for line in lines:\n",
    "        box_id = line.split()[0]\n",
    "        cls_name = ' '.join(line.split()[1:])\n",
    "        feature_id = cls_name + '_' + box_id\n",
    "        box_img = box_path + im_id + '/'+box_id+ '.jpg'\n",
    "        box_img_pil = Image.open(box_img)\n",
    "\n",
    "        inputs = feature_extractor(box_img_pil, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = vit_model(**inputs)\n",
    "\n",
    "        pool = outputs.pooler_output\n",
    "#         print(pool.shape,feature_id) 1,768 dark sky_0\n",
    "        features.append(pool)\n",
    "    \n",
    "    features = torch.cat(features,0)\n",
    "    return features\n",
    "\n",
    "def vision_10_boxes_embedding(video_id):\n",
    "    '''\n",
    "    return 10 * 768 and 10 box_text\n",
    "    '''\n",
    "    frame_id_path = frame_path + video_id\n",
    "    frames = glob(frame_id_path + '/*')\n",
    "    frames = sorted(frames)\n",
    "    # 00001.jpg\n",
    "    frame0 = frames[0]\n",
    "    box_list = file_2_10_boxes_list(frame0)\n",
    "    \n",
    "    generate_10_box_pics(video_id, frame0, box_list)\n",
    "    \n",
    "    box_feature768 = vit_feature(video_id)\n",
    "    \n",
    "    \n",
    "    cls_names = [box[1] for box in box_list]\n",
    "    # print(cls_names)\n",
    "    \n",
    "    return box_feature768, cls_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550957d-27e4-422b-a422-f87390cee9de",
   "metadata": {},
   "source": [
    "# 生成全局视觉特征768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e0ec6-401b-406c-9889-48def6b72007",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import ViTFeatureExtractor,ViTModel\n",
    "from PIL import Image\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
    "    \"vit-base-patch16-224-in21k\"\n",
    ")\n",
    "vit_model = ViTModel.from_pretrained(\"vit-base-patch16-224-in21k\")\n",
    "\n",
    "\n",
    "def vision_full_pic_embedding(video_id):\n",
    "    '''\n",
    "    return full pic vit model 768 \n",
    "    '''\n",
    "    frame_id_path = frame_path + video_id\n",
    "    frames = glob(frame_id_path + '/*')\n",
    "    full_feature768 = []\n",
    "    for frame in sorted(frames):\n",
    "        # print(frame)\n",
    "        img = Image.open(frame)\n",
    "        inputs = feature_extractor(img, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = vit_model(**inputs)\n",
    "\n",
    "        pool = outputs.pooler_output\n",
    "        full_feature768.append(pool)\n",
    "        \n",
    "        # break\n",
    "    full_feature768 = torch.cat(full_feature768, 0 )\n",
    "    # print(full_feature768.size())\n",
    "    \n",
    "    return full_feature768\n",
    "\n",
    "vision_full_feats = []\n",
    "for index in (range(len(df))):\n",
    "    video_id, _, text, label, _, mode, _ = df.loc[index]\n",
    "    print(index)\n",
    "    f = vision_full_pic_embedding(video_id)\n",
    "    vision_full_feats.append(f)\n",
    "    if index == 1 :\n",
    "        pass\n",
    "        # break\n",
    "\n",
    "print(len(vision_full_feats)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cffe9-53c2-473c-936a-19f1374ff994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def func_padding_vision_full_feature():\n",
    "    max_vision_len = 0\n",
    "    vision_feature_dim = vision_full_feats[0].size(1)\n",
    "    video_nums = len(vision_full_feats)\n",
    "    for vision in vision_full_feats:\n",
    "        if vision.size(0) > max_vision_len:\n",
    "            max_vision_len = vision.size(0)\n",
    "\n",
    "    padding_vision_full_feature = torch.zeros(video_nums, max_vision_len,vision_feature_dim)\n",
    "    for index, vision in enumerate(vision_full_feats):\n",
    "        frames_num = vision.size(0)\n",
    "        padding_vision_full_feature[index][:frames_num] = vision\n",
    "        \n",
    "    return padding_vision_full_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d165a-166d-4237-b682-e146896f13b3",
   "metadata": {},
   "source": [
    "# generate_graph(text) generate_cross_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd25673-522a-4771-85c4-f5fda37f0d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_graph(text):\n",
    "    from transformers import BertTokenizer\n",
    "    from collections import defaultdict\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"./bert-base-uncased/\")\n",
    "    \n",
    "    bert_token = tokenizer.tokenize(text)\n",
    "    document = nlp(text)\n",
    "    spacy_token = [str(x) for x in document]\n",
    "    spacy_len = len(spacy_token)\n",
    "    bert_len = len(bert_token)\n",
    "    \n",
    "    ii = 0\n",
    "    jj = 0\n",
    "    s = \"\"\n",
    "    pre = []\n",
    "    split_link = []\n",
    "    while ii < bert_len and jj < spacy_len:\n",
    "        b = bert_token[ii].replace('##','')\n",
    "        s += b\n",
    "        pre.append(ii)\n",
    "        spa_ = spacy_token[jj]\n",
    "        # print(s, ' =? ', spa_)\n",
    "        if s == spa_:\n",
    "            split_link.append(pre)\n",
    "            jj += 1\n",
    "            s = \"\"\n",
    "            pre = []\n",
    "\n",
    "        ii += 1\n",
    "    # print(text)\n",
    "    # print(bert_token)\n",
    "    # print(spacy_token)\n",
    "    # print(split_link)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    mat = defaultdict(list,[])\n",
    "    for t in doc:\n",
    "        for child in t.children:\n",
    "            # print(t, t.i, child.i)\n",
    "            mat[child.i].append(t.i)\n",
    "            mat[t.i].append(child.i)\n",
    "    \n",
    "    import numpy as np\n",
    "    outter_graph = np.zeros((bert_len,bert_len)).astype('float32')\n",
    "    \n",
    "    for key,linked in mat.items():\n",
    "        for x in split_link[key]:\n",
    "            for link in linked:\n",
    "                for y in split_link[link]:\n",
    "                    outter_graph[x][y] = 1\n",
    "                \n",
    "    tokens = bert_token\n",
    "    inner_graph = np.identity(bert_len).astype('float32')\n",
    "    for link in split_link:\n",
    "        for x in link:\n",
    "            for y in link:\n",
    "                inner_graph[x][y] = 1\n",
    "    \n",
    "    outter_graph = np.pad(outter_graph, ((1,1),(1,1)), 'constant')\n",
    "    inner_graph = np.pad(inner_graph, ((1,1),(1,1)), 'constant')\n",
    "    inner_graph[0][0] = 1\n",
    "    inner_graph[-1][-1] = 1\n",
    "    graph1 = inner_graph + outter_graph\n",
    "                \n",
    "    # print(graph1, graph1.shape)\n",
    "    return graph1\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-uncased/')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_split(text):\n",
    "    split_link = []\n",
    "    bert_token = tokenizer.tokenize(text)\n",
    "    document = nlp(text)\n",
    "    spacy_token = [str(x) for x in document]\n",
    "    bert_len = len(bert_token)\n",
    "    spacy_len = len(spacy_token)\n",
    "    ii = 0\n",
    "    jj = 0\n",
    "    pre = []\n",
    "    s = \"\"\n",
    "    while ii < bert_len and jj < spacy_len:\n",
    "        bert_ = bert_token[ii].replace('##','')\n",
    "        s += bert_\n",
    "        pre.append(ii)\n",
    "        spacy_ = spacy_token[jj]\n",
    "        if s == spacy_:\n",
    "            split_link.append(pre)\n",
    "            pre = []\n",
    "            s = \"\"\n",
    "            jj += 1\n",
    "        ii += 1\n",
    "    print(spacy_token)\n",
    "    print(bert_token)\n",
    "    return split_link\n",
    "\n",
    "def load_sentic_word():\n",
    "    path = './senticNet/senticnet_word.txt'\n",
    "    sNet = {}\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, score = line.split('\\t')\n",
    "            sNet[word] = float(score)\n",
    "    return sNet\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "y = 3\n",
    "\n",
    "def get_sentic_score(si, adj):\n",
    "    '''\n",
    "    计算情感的不一致性，一致设置为0 ，否则放大这种不协调，讽刺的冲突\n",
    "    '''\n",
    "    word_i = si.lemma_names()[0]\n",
    "    word_j = adj.lemma_names()[0]\n",
    "    if word_i not in senticNet or word_j not in senticNet or \\\n",
    "      word_i == word_j:\n",
    "        res = 0\n",
    "    else:\n",
    "        res = abs(senticNet[word_i] - senticNet[word_j]) * \\\n",
    "          y ** (-1 * senticNet[word_i] * senticNet[word_j])\n",
    "    \n",
    "    # print(word_i, word_j, res)\n",
    "    return res\n",
    "\n",
    "senticNet = load_sentic_word()\n",
    "list(senticNet.items())[:2]\n",
    "\n",
    "def generate_cross_graph(text, box_text):\n",
    "    # text = 'haha . lol'\n",
    "    # text = 'thanks for showing up for our appointment today .'\n",
    "    \n",
    "    bert_text_tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    \n",
    "    document = nlp(text.lower())\n",
    "    spacy_text_tokens = [str(x).lower() for x in document]\n",
    "    spacy_text_tokens\n",
    "    box_len = len(box_text)\n",
    "    box_len\n",
    "    import numpy as np\n",
    "    graph = np.zeros((len(bert_text_tokens), box_len)).astype('float32')\n",
    "    graph, graph.shape\n",
    "    split_link_ = get_split(text)\n",
    "    split_link_\n",
    "    split_link = dict()\n",
    "    for idx, value in enumerate(split_link_):\n",
    "        for v in value:\n",
    "            split_link[bert_text_tokens[v]] = spacy_text_tokens[idx]\n",
    "    split_link\n",
    "    \n",
    "    for i, token in enumerate(bert_text_tokens):\n",
    "        cur = 0\n",
    "        sp_token = split_link[token]\n",
    "        si = wn.synsets(sp_token)\n",
    "        if len(si) == 0 :\n",
    "            continue\n",
    "        si = si[0]\n",
    "        for b in box_text:\n",
    "            tokens_ = nlp(b)\n",
    "            tokens_ = [str(x) for x in tokens_]\n",
    "    #         print(tokens_)\n",
    "            if len(tokens_) == 2:\n",
    "                adj_j, obj_j = tokens_\n",
    "            if len(tokens_) == 1:\n",
    "                adj_j, obj_j = 'a', tokens_[0]\n",
    "            adj = wn.synsets(adj_j)[0]\n",
    "            obj = wn.synsets(obj_j)[0]\n",
    "            sim = wn.path_similarity(si, obj)\n",
    "            if sim is None:\n",
    "                graph[i][cur] = 0 + get_sentic_score(si, adj)\n",
    "            else:\n",
    "                graph[i][cur] = sim + get_sentic_score(si, adj)\n",
    "            # print(i, cur, graph[i][cur])\n",
    "            cur += 1\n",
    "    # print(graph, graph.shape)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd645e6-b29d-4674-931e-db4a9ad5da4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 调试生成特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfe2d9-b518-4ec2-a705-1a19a54f54fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index in (range(len(df))):\n",
    "    if index == 1:\n",
    "        video_id, _, text, label, _, mode, _ = df.loc[index]\n",
    "        # graph = generate_graph(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7a690-7d76-465f-a964-ef26626f79e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 生成多模态特征数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7790e2-22d8-41b6-916b-20c2ad9fcdf3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_features = []\n",
    "modes = []\n",
    "labels = []\n",
    "video_features = []\n",
    "box_texts =[]\n",
    "graphs = []\n",
    "cross_graphs = []\n",
    "texts = []\n",
    "for index in tqdm(range(len(df))):\n",
    "    video_id, _, text, label, _, mode, _ = df.loc[index]\n",
    "    audio_dir = audios_path + video_id\n",
    "    audio_file = audio_dir + '/tmp.wav'\n",
    "    video_file = video_path + video_id + '.mp4'\n",
    "    if not os.path.exists(audio_dir):\n",
    "        os.makedirs(audio_dir)\n",
    "        print('makedirs ' + audio_dir)\n",
    "        video_to_wav(video_file, audio_file)\n",
    "    audio_feature = audio_embedding(audio_file)\n",
    "    \n",
    "    # vision\n",
    "    video_feature, cls_names_10 = vision_10_boxes_embedding(video_id)\n",
    "    graph = generate_graph(text)\n",
    "    cross_graph = generate_cross_graph(text, cls_names_10)\n",
    "    \n",
    "    texts.append(text)\n",
    "    audio_features.append(audio_feature)\n",
    "    video_features.append(video_feature)\n",
    "    box_texts.append(cls_names_10)\n",
    "    graphs.append(graph)\n",
    "    cross_graphs.append(cross_graph)\n",
    "    modes.append(mode)\n",
    "    labels.append(label)\n",
    "    \n",
    "    if index == 1:\n",
    "        pass\n",
    "        # break\n",
    "print(len(box_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb87bb0-6a6b-4f7a-8777-763b60f9a975",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 填充函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9cdc62-2c30-45a2-937d-ca5292f32a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert-base-uncased/\")\n",
    "\n",
    "\n",
    "def merge_graph(graphs, cross_graphs):\n",
    "    bert_indices_max_len = 0\n",
    "    bert_indices_len = []\n",
    "    for graph, cross_graph in zip(graphs, cross_graphs):\n",
    "        bert_indices_len.append(graph.shape[0])\n",
    "    bert_indices_max_len = max(bert_indices_len)\n",
    "    \n",
    "    big_graphs = []\n",
    "    for graph, cross_graph in zip(graphs, cross_graphs):\n",
    "        cross_graph = np.pad(cross_graph, ((1,1),(0,0)), 'constant')\n",
    "        # print(graph.shape, cross_graph.shape)\n",
    "        if graph.shape[0] < bert_indices_max_len:\n",
    "            graph = np.pad(graph, ((0,bert_indices_max_len-graph.shape[0]),(0,bert_indices_max_len-graph.shape[0])), 'constant')\n",
    "        print(graph.shape, cross_graph.shape)\n",
    "        \n",
    "        graph = np.pad(graph,((0,10),(0,10)),'constant')\n",
    "        print(graph.shape)\n",
    "        image_graph = cross_graph\n",
    "        for i in range(image_graph.shape[0]-2):\n",
    "            for j in range(image_graph.shape[1]):\n",
    "                if not np.isnan(image_graph[i][j]):\n",
    "                    # print(i+1,j+bert_indices_max_len,'|',i+1,j)\n",
    "                    graph[i+1][j+bert_indices_max_len] = image_graph[i + 1][j] + 1 \n",
    "                    graph[j+bert_indices_max_len][i+1] = image_graph[i + 1][j] + 1 \n",
    "                else:\n",
    "                    graph[i+1][j+bert_indices_max_len] =  1\n",
    "                    graph[j+bert_indices_max_len][i+1] =  1\n",
    "\n",
    "        for i in range(image_graph.shape[1]):\n",
    "            graph[i+bert_indices_max_len][i+bert_indices_max_len] = 1 \n",
    "        graph = np.expand_dims(graph, axis=0)\n",
    "        big_graphs.append(graph)\n",
    "            \n",
    "    big_graphs = np.concatenate(big_graphs, axis=0)\n",
    "    \n",
    "    return big_graphs\n",
    "\n",
    "\n",
    "\n",
    "def labels_to_np(labels):\n",
    "    labels = np.array(labels)\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def padding_video_features(video_features):\n",
    "    features_p = []\n",
    "    for vf in video_features:\n",
    "        vf = np.expand_dims(vf, axis=0)\n",
    "        features_p.append(vf)\n",
    "    features_p = np.concatenate(features_p, axis = 0)\n",
    "    return features_p\n",
    "\n",
    "\n",
    "\n",
    "def padding_text(texts):\n",
    "    bert_indices = []\n",
    "    for text in texts:\n",
    "        bert_tokens = ['[CLS]'] + tokenizer.tokenize(text) + ['[SEP]']\n",
    "        bert_index = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "        bert_indices.append(bert_index)\n",
    "        # print(bert_index)\n",
    "    lens = [len(x) for x in bert_indices]\n",
    "    bert_indices_max_len = max(lens)\n",
    "    \n",
    "    bert_indices_pad = [np.pad(x,(0, bert_indices_max_len - len(x)),'constant') for x in bert_indices]\n",
    "    bert_indices_pad = np.array(bert_indices_pad)\n",
    "    \n",
    "    # batch_bert_indices.append(numpy.pad(bert_indices,(0,bert_indices_max_len - len(bert_indices)),'constant'))\n",
    "    print(bert_indices_pad.shape)\n",
    "    return bert_indices_pad\n",
    "\n",
    "\n",
    "def padding_box_text(box_texts):\n",
    "    box_bert_indices = []\n",
    "    for box_text in box_texts:\n",
    "        # print(box_text)\n",
    "        for text in box_text:\n",
    "            bert_tokens = tokenizer.tokenize(text) \n",
    "            bert_index = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "            box_bert_indices.append(bert_index)\n",
    "        \n",
    "    lens = [len(x) for x in box_bert_indices]\n",
    "    box_indices_max_len = max(lens)\n",
    "    \n",
    "    box_pad_indices = []\n",
    "    for box_text in box_texts:\n",
    "        new_box_indices = []\n",
    "        box_indices = []\n",
    "        for text in box_text:\n",
    "            bert_tokens = tokenizer.tokenize(text) \n",
    "            bert_index = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "            box_indices.append(bert_index)\n",
    "            \n",
    "        for box_indice in box_indices:\n",
    "            if len(box_indice) < box_indices_max_len:\n",
    "                box_indice = box_indice + [0]*(box_indices_max_len - len(box_indice))\n",
    "            new_box_indices.append(np.array(box_indice))\n",
    "\n",
    "        while len(new_box_indices) < 10:\n",
    "            new_box_indices.append([0]*box_indices_max_len)\n",
    "        box_pad_indices.append(new_box_indices)\n",
    "    \n",
    "    box_pad_indices = np.array(box_pad_indices)\n",
    "    \n",
    "    print(box_pad_indices.shape)\n",
    "    return box_pad_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92d572-117a-464d-8411-51a6f87e7d5f",
   "metadata": {},
   "source": [
    "# 填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cea40-10e3-4bc2-87d9-cde1aa1bcc26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "big_graphs = merge_graph(graphs, cross_graphs)\n",
    "labels = labels_to_np(labels)\n",
    "video_features_p = padding_video_features(video_features)\n",
    "bert_indices = padding_text(texts)\n",
    "box_pad_indices = padding_box_text(box_texts)\n",
    "audio_padding_fea, audio_length_array = padding_sequence(audio_features)\n",
    "padding_vision_full_feature = func_padding_vision_full_feature()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aa58cb-5e85-4e5d-bbe8-849228367b6b",
   "metadata": {},
   "source": [
    "# 整理数据 train valid test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19fcae2-a0a9-414f-b24d-487e51c70c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "inx_dict = { mode + '_index' : [\n",
    "    i for i,v in enumerate(modes) if v==mode\n",
    "] for mode in ['train', 'valid', 'test']}\n",
    "\n",
    "\n",
    "final_data = {k:{} for k in ['train', 'valid', 'test']}\n",
    "for mode in ['train', 'valid', 'test']:\n",
    "    indexes = inx_dict[mode + '_index']\n",
    "    final_data[mode]['audio_feature'] = audio_padding_fea[indexes]\n",
    "    final_data[mode]['video_features_p'] = video_features_p[indexes]\n",
    "    final_data[mode]['bert_indices'] = bert_indices[indexes]\n",
    "    final_data[mode]['box_pad_indices'] = box_pad_indices[indexes]\n",
    "    final_data[mode]['big_graphs'] = big_graphs[indexes]\n",
    "    final_data[mode]['labels'] = labels[indexes]\n",
    "    final_data[mode]['vision_full_feature'] = padding_vision_full_feature[indexes]\n",
    "    \n",
    "\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(final_data, f)\n",
    "print('write to', output_path)\n",
    "get_file_size(output_path, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5951a-7b74-4f11-8ba7-941089422665",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_features_p.shape, padding_vision_full_feature.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
