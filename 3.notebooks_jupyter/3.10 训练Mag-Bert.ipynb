{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442b0294-3329-4c41-aca8-075065ba6bf5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4607b4ca-8fba-403d-983d-d6e65e29d50e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio_feature', 'video_features_p', 'bert_indices', 'box_pad_indices', 'big_graphs', 'labels', 'index'])\n",
      "33\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dataset.py\n",
    "'''\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class MUStartDataset(Dataset):\n",
    "    def __init__(self,mode = 'train',feature_path = './featuresIndepVit768.pkl'):\n",
    "        with open(feature_path,'rb') as f:\n",
    "            import pickle\n",
    "            data = pickle.load(f)\n",
    "        self.feature_dict = data[mode]\n",
    "        # [-1,1] -> [0,1]\n",
    "        self.feature_dict['labels'] = ((self.feature_dict['labels'] + 1)/2).astype(np.int64)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        feature ={}\n",
    "        feature['audio_feature'] = self.feature_dict['audio_feature'][index]\n",
    "        feature['video_features_p'] = self.feature_dict['video_features_p'][index]\n",
    "        feature['bert_indices'] = self.feature_dict['bert_indices'][index]\n",
    "        feature['box_pad_indices'] = self.feature_dict['box_pad_indices'][index]\n",
    "        feature['big_graphs'] = self.feature_dict['big_graphs'][index]\n",
    "        feature['labels'] = self.feature_dict['labels'][index]\n",
    "        feature['index'] = index\n",
    "        \n",
    "        return feature\n",
    "    def __len__(self):\n",
    "        labels = self.feature_dict['labels']\n",
    "        length = labels.shape[0]\n",
    "        return length\n",
    "    \n",
    "    def get_sample_shape(self,index):\n",
    "        shape_dict = {}\n",
    "        shape_dict['audio_feature'] = self.feature_dict['audio_feature'][index].shape\n",
    "        shape_dict['video_features_p'] = self.feature_dict['video_features_p'][index].shape\n",
    "        shape_dict['bert_indices'] = self.feature_dict['bert_indices'][index].shape\n",
    "        shape_dict['box_pad_indices'] = self.feature_dict['box_pad_indices'][index].shape\n",
    "        shape_dict['big_graphs'] = self.feature_dict['big_graphs'][index].shape\n",
    "        # shape_dict['labels'] = self.feature_dict['labels'][index].shape\n",
    "        shape_dict['labels'] = type(self.feature_dict['labels'][index])\n",
    "        return shape_dict\n",
    "        \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    d = MUStartDataset('valid')\n",
    "    dl = DataLoader(d, batch_size=2, num_workers=0, shuffle=False)\n",
    "    batch_sample = iter(dl).next()\n",
    "    print(batch_sample.keys())\n",
    "    print(batch_sample['audio_feature'].size(2))\n",
    "    print(batch_sample['video_features_p'].size(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023624a-a495-4f82-b720-bd2494bb7e7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 准备模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5e39db-19cc-4ae2-bc05-c56bf5c2acdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "\n",
    "class MAG(nn.Module):\n",
    "    def __init__(self, hidden_size, beta_shift, dropout_prob):        \n",
    "        super(MAG, self).__init__()\n",
    "        print(\"Initializing MAG with beta_shift:{} hidden_prob:{}\".format(beta_shift, dropout_prob))\n",
    "\n",
    "        self.W_hv = nn.Linear(VISUAL_DIM + TEXT_DIM, TEXT_DIM)\n",
    "        self.W_ha = nn.Linear(ACOUSTIC_DIM + TEXT_DIM, TEXT_DIM)\n",
    "        self.W_v = nn.Linear(VISUAL_DIM, TEXT_DIM)\n",
    "        self.W_a = nn.Linear(ACOUSTIC_DIM, TEXT_DIM)\n",
    "        self.beta_shift = beta_shift\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, text_embedding, visual, acoustic):\n",
    "        eps = 1e-6\n",
    "        weight_v = F.relu(self.W_hv(torch.cat((visual, text_embedding), dim=-1)))\n",
    "        weight_a = F.relu(self.W_ha(torch.cat((acoustic, text_embedding), dim=-1)))\n",
    "        h_m = weight_v * self.W_v(visual) + weight_a * self.W_a(acoustic)\n",
    "        em_norm = text_embedding.norm(2, dim=-1)\n",
    "        hm_norm = h_m.norm(2, dim=-1)\n",
    "        DEVICE = visual.device\n",
    "        hm_norm_ones = torch.ones(hm_norm.shape, requires_grad=True).to(DEVICE)\n",
    "        hm_norm = torch.where(hm_norm == 0, hm_norm_ones, hm_norm)\n",
    "        thresh_hold = (em_norm / (hm_norm + eps)) * self.beta_shift\n",
    "        ones = torch.ones(thresh_hold.shape, requires_grad=True).to(DEVICE)\n",
    "        alpha = torch.min(thresh_hold, ones)\n",
    "        alpha = alpha.unsqueeze(dim=-1)\n",
    "        acoustic_vis_embedding = alpha * h_m\n",
    "        embedding_output = self.dropout(\n",
    "            self.LayerNorm(acoustic_vis_embedding + text_embedding)\n",
    "        )\n",
    "\n",
    "        return embedding_output\n",
    "\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertEncoder, BertPooler\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class MultimodalConfig(object):\n",
    "    def __init__(self, beta_shift, dropout_prob):\n",
    "        self.beta_shift = beta_shift\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "class MAG_BertModel(BertPreTrainedModel):\n",
    "    def __init__(self, config, multimodal_config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.MAG = MAG(\n",
    "            config.hidden_size,\n",
    "            multimodal_config.beta_shift,\n",
    "            multimodal_config.dropout_prob,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "    self,\n",
    "    input_ids,\n",
    "    visual,\n",
    "    acoustic,\n",
    "    attention_mask=None,\n",
    "    token_type_ids=None,\n",
    "    position_ids=None,\n",
    "    head_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    singleTask = False,\n",
    "    ):\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        fused_embedding = self.MAG(embedding_output, visual, acoustic)\n",
    "        \n",
    "        encoder_outputs = self.encoder(\n",
    "            fused_embedding,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
    "            1:\n",
    "        ]  # add hidden_states and attentions if they are here\n",
    "        # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        return outputs\n",
    "        \n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias :\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias',None)\n",
    "        \n",
    "    def forward(self, text, adj):\n",
    "        hidden = torch.matmul(text,self.weight)\n",
    "        \n",
    "        denom = torch.sum(adj,dim=2,keepdim=True) + 1\n",
    "        output = torch.matmul(adj, hidden.float())/denom\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "\n",
    "        return output\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class AlignSubNet(nn.Module):\n",
    "    def __init__(self, dst_len):\n",
    "        \"\"\"\n",
    "        mode: the way of aligning avg_pool 这个模型并没有参数\n",
    "        \"\"\"\n",
    "        super(AlignSubNet, self).__init__()\n",
    "        self.dst_len = dst_len\n",
    "\n",
    "    def get_seq_len(self):\n",
    "        return self.dst_len\n",
    "    \n",
    "    def __avg_pool(self, text_x, audio_x, video_x):\n",
    "        def align(x):\n",
    "            raw_seq_len = x.size(1)\n",
    "            if raw_seq_len == self.dst_len:\n",
    "                return x\n",
    "            if raw_seq_len // self.dst_len == raw_seq_len / self.dst_len:\n",
    "                pad_len = 0\n",
    "                pool_size = raw_seq_len // self.dst_len\n",
    "            else:\n",
    "                pad_len = self.dst_len - raw_seq_len % self.dst_len\n",
    "                pool_size = raw_seq_len // self.dst_len + 1\n",
    "            pad_x = x[:, -1, :].unsqueeze(1).expand([x.size(0), pad_len, x.size(-1)])\n",
    "            x = torch.cat([x, pad_x], dim=1).view(x.size(0), pool_size, self.dst_len, -1)\n",
    "            x = x.mean(dim=1)\n",
    "            return x\n",
    "        text_x = align(text_x)\n",
    "        audio_x = align(audio_x)\n",
    "        video_x = align(video_x)\n",
    "        return text_x, audio_x, video_x\n",
    "    \n",
    " \n",
    "    def forward(self, text_x, audio_x, video_x):\n",
    "        if text_x.size(1) == audio_x.size(1) == video_x.size(1):\n",
    "            return text_x, audio_x, video_x\n",
    "        return self.__avg_pool(text_x, audio_x, video_x)\n",
    "\n",
    "    \n",
    "class CMGCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CMGCN, self).__init__()\n",
    "        print('create CMGCN model')\n",
    "        self.bert = BertModel.from_pretrained('./bert-base-uncased/')\n",
    "        self.text_lstm = DynamicLSTM(768,4,num_layers=1,batch_first=True,bidirectional=True)\n",
    "        self.vit_fc = nn.Linear(768,2*4)\n",
    "        self.gc1 = GraphConvolution(2*4, 2*4)\n",
    "        self.gc2 = GraphConvolution(2*4, 2*4)\n",
    "        self.fc = nn.Linear(2*4,2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bert_indices = inputs['bert_indices']\n",
    "        graph = inputs['big_graphs']\n",
    "        box_vit = inputs['video_features_p']\n",
    "        bert_text_len = torch.sum(bert_indices != 0, dim = -1)\n",
    "        outputs = self.bert(bert_indices)\n",
    "        encoder_layer = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        text_out, (_, _) = self.text_lstm(encoder_layer, bert_text_len)\n",
    "        # 与原始代码不同，这里因为进行了全局的特征填充，导致text_out可能无法达到填充长度，补充为0\n",
    "        if text_out.shape[1] < encoder_layer.shape[1]:\n",
    "            pad = torch.zeros((text_out.shape[0],encoder_layer.shape[1]-text_out.shape[1],text_out.shape[2]))\n",
    "            text_out = torch.cat((text_out,pad),dim=1)\n",
    "\n",
    "        box_vit = box_vit.float()\n",
    "        box_vit = self.vit_fc(box_vit)\n",
    "        features = torch.cat([text_out, box_vit], dim=1)\n",
    "\n",
    "        graph = graph.float()\n",
    "        x = F.relu(self.gc1(features, graph))\n",
    "        x = F.relu(self.gc2(x,graph))\n",
    "        \n",
    "        alpha_mat = torch.matmul(features,x.transpose(1,2))\n",
    "        alpha_mat = alpha_mat.sum(1, keepdim=True)\n",
    "        alpha = F.softmax(alpha_mat, dim = 2)\n",
    "        x = torch.matmul(alpha, x).squeeze(1)\n",
    "        \n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class CMGCNI(nn.Module):\n",
    "    def __init__(self, multimodal_config):\n",
    "        super(CMGCNI, self).__init__()\n",
    "        print('create CMGCNI model')\n",
    "        self.mag_bert = MAG_BertModel.from_pretrained('./bert-base-uncased/',multimodal_config=multimodal_config)\n",
    "        self.text_lstm = DynamicLSTM(768,4,num_layers=1,batch_first=True,bidirectional=True)\n",
    "        self.vit_fc = nn.Linear(768,2*4)\n",
    "        self.gc1 = GraphConvolution(2*4, 2*4)\n",
    "        self.gc2 = GraphConvolution(2*4, 2*4)\n",
    "        self.fc = nn.Linear(2*4,2)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bert_indices = inputs['bert_indices']\n",
    "        graph = inputs['big_graphs']\n",
    "        box_vit = inputs['video_features_p']\n",
    "        bert_text_len = torch.sum(bert_indices != 0, dim = -1)\n",
    "        # 2,24, audio_feature key 2 33 33 , 2,10 768 \n",
    "        visual = box_vit\n",
    "        acoustic = inputs['audio_feature']\n",
    "        self.align_subnet = AlignSubNet(bert_indices.size(1))\n",
    "        bert_indices, acoustic, visual= self.align_subnet(bert_indices,acoustic,visual)\n",
    "        acoustic = acoustic.float()\n",
    "        outputs = self.mag_bert(bert_indices, visual, acoustic)\n",
    "        \n",
    "        encoder_layer = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        text_out, (_, _) = self.text_lstm(encoder_layer, bert_text_len)\n",
    "        # 与原始代码不同，这里因为进行了全局的特征填充，导致text_out可能无法达到填充长度，补充为0\n",
    "        if text_out.shape[1] < encoder_layer.shape[1]:\n",
    "            pad = torch.zeros((text_out.shape[0],encoder_layer.shape[1]-text_out.shape[1],text_out.shape[2]))\n",
    "            text_out = torch.cat((text_out,pad),dim=1)\n",
    "\n",
    "        box_vit = box_vit.float()\n",
    "        box_vit = self.vit_fc(box_vit)\n",
    "        features = torch.cat([text_out, box_vit], dim=1)\n",
    "\n",
    "        graph = graph.float()\n",
    "        x = F.relu(self.gc1(features, graph))\n",
    "        x = F.relu(self.gc2(x,graph))\n",
    "        \n",
    "        alpha_mat = torch.matmul(features,x.transpose(1,2))\n",
    "        alpha_mat = alpha_mat.sum(1, keepdim=True)\n",
    "        alpha = F.softmax(alpha_mat, dim = 2)\n",
    "        x = torch.matmul(alpha, x).squeeze(1)\n",
    "        \n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "class MagBertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, multimodal_config):\n",
    "        super(MagBertForSequenceClassification, self).__init__()\n",
    "        print('create MagBertForSequenceClassification model')\n",
    "        self.mag_bert = MAG_BertModel.from_pretrained('./bert-base-uncased/',multimodal_config=multimodal_config)\n",
    "        self.dropout = nn.Dropout(0.1) # bert config 中的设置\n",
    "        self.classifier = nn.Linear(768,2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bert_indices = inputs['bert_indices']\n",
    "        box_vit = inputs['video_features_p']\n",
    "        bert_text_len = torch.sum(bert_indices != 0, dim = -1)\n",
    "        # 2,24, audio_feature key 2 33 33 , 2,10 768 \n",
    "        visual = box_vit\n",
    "        acoustic = inputs['audio_feature']\n",
    "        self.align_subnet = AlignSubNet(bert_indices.size(1))\n",
    "        bert_indices, acoustic, visual= self.align_subnet(bert_indices,acoustic,visual)\n",
    "\n",
    "        acoustic = acoustic.float()\n",
    "        \n",
    "        outputs = self.mag_bert(bert_indices, visual, acoustic)\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        output = logits\n",
    "        return output\n",
    "    \n",
    "class EF_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EF_LSTM, self).__init__()\n",
    "        print('create EF_LSTM model')\n",
    "        self.bert = BertModel.from_pretrained('./bert-base-uncased/')\n",
    "        self.norm = nn.BatchNorm1d(TEXT_SEQ_LEN)\n",
    "        self.lstm = nn.LSTM(ACOUSTIC_DIM+VISUAL_DIM+TEXT_DIM, 64, num_layers=2, dropout=0.3, bidirectional=False, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(64,64)\n",
    "        self.out = nn.Linear(64,2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bert_indices = inputs['bert_indices']\n",
    "        # graph = inputs['big_graphs']\n",
    "        box_vit = inputs['video_features_p']\n",
    "        bert_text_len = torch.sum(bert_indices != 0, dim = -1)\n",
    "        # 2,24, audio_feature key 2 33 33 , 2,10 768 \n",
    "        visual = box_vit\n",
    "        acoustic = inputs['audio_feature']\n",
    "        self.align_subnet = AlignSubNet(bert_indices.size(1))\n",
    "        bert_indices, acoustic, visual= self.align_subnet(bert_indices,acoustic,visual)\n",
    "        acoustic = acoustic.float()\n",
    "        outputs = self.bert(bert_indices)\n",
    "        encoder_layer = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        text_x = encoder_layer\n",
    "        audio_x = acoustic\n",
    "        video_x = visual\n",
    "        x = torch.cat([text_x, audio_x, video_x], dim=-1)\n",
    "        x = self.norm(x)\n",
    "        _, final_states = self.lstm(x)\n",
    "        x = self.dropout(final_states[0][-1].squeeze(dim=0))\n",
    "        x = F.relu(self.linear(x), inplace=True)\n",
    "        x = self.dropout(x)\n",
    "        output = self.out(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "from models.subNets.FeatureNets import SubNet,TextSubNet\n",
    "\n",
    "class LF_DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LF_DNN, self).__init__()\n",
    "        print('create LF_DNN model')\n",
    "        self.bert = BertModel.from_pretrained('./bert-base-uncased/')\n",
    "        self.audio_in = ACOUSTIC_DIM\n",
    "        self.video_in = VISUAL_DIM\n",
    "        self.text_in = TEXT_DIM\n",
    "        self.audio_hidden = 8\n",
    "        self.video_hidden = 64\n",
    "        self.text_hidden = 64\n",
    "        self.text_out = 32\n",
    "        self.post_fusion_dim = 32\n",
    "        self.audio_prob, self.video_prob, self.text_prob, self.post_fusion_prob = 0.2, 0.2, 0.2, 0.2\n",
    "        self.audio_subnet = SubNet(self.audio_in, self.audio_hidden, self.audio_prob)\n",
    "        self.video_subnet = SubNet(self.video_in, self.video_hidden, self.video_prob)\n",
    "        self.text_subnet = TextSubNet(self.text_in, self.text_hidden, self.text_out, dropout=self.text_prob)\n",
    "\n",
    "        self.post_fusion_dropout = nn.Dropout(p=self.post_fusion_prob)\n",
    "        self.post_fusion_layer_1 = nn.Linear(self.text_out + self.video_hidden + self.audio_hidden,\n",
    "                                             self.post_fusion_dim)\n",
    "        self.post_fusion_layer_2 = nn.Linear(self.post_fusion_dim, self.post_fusion_dim)\n",
    "        self.post_fusion_layer_3 = nn.Linear(self.post_fusion_dim, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bert_indices = inputs['bert_indices']\n",
    "        box_vit = inputs['video_features_p']\n",
    "        bert_text_len = torch.sum(bert_indices != 0, dim = -1)\n",
    "        # 2,24, audio_feature key 2 33 33 , 2,10 768 \n",
    "        visual = box_vit\n",
    "        acoustic = inputs['audio_feature']\n",
    "        self.align_subnet = AlignSubNet(bert_indices.size(1))\n",
    "        bert_indices, acoustic, visual= self.align_subnet(bert_indices,acoustic,visual)\n",
    "        acoustic = acoustic.float()\n",
    "        outputs = self.bert(bert_indices)\n",
    "        encoder_layer = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        text_x = encoder_layer\n",
    "        audio_x = acoustic\n",
    "        video_x = visual\n",
    "        audio_x = torch.mean(audio_x,1,True)\n",
    "        video_x = torch.mean(video_x,1,True)\n",
    "        audio_x[audio_x != audio_x] = 0\n",
    "        video_x[video_x != video_x] = 0\n",
    "        audio_x = audio_x.squeeze(1)\n",
    "        video_x = video_x.squeeze(1)\n",
    "        \n",
    "        audio_h = self.audio_subnet(audio_x)\n",
    "        video_h = self.video_subnet(video_x)\n",
    "        text_h = self.text_subnet(text_x)\n",
    "        \n",
    "        fusion_h = torch.cat([audio_h, video_h, text_h], dim=-1)\n",
    "\n",
    "        x = self.post_fusion_dropout(fusion_h)\n",
    "        x = F.relu(self.post_fusion_layer_1(x), inplace=True)\n",
    "        x = F.relu(self.post_fusion_layer_2(x), inplace=True)\n",
    "        output = self.post_fusion_layer_3(x)\n",
    "               \n",
    "        return output\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "class AuViSubNet(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size, num_layers=1, dropout=0.2, bidirectional=False):\n",
    "        '''\n",
    "        Args:\n",
    "            in_size: input dimension\n",
    "            hidden_size: hidden layer dimension\n",
    "            num_layers: specify the number of layers of LSTMs.\n",
    "            dropout: dropout probability\n",
    "            bidirectional: specify usage of bidirectional LSTM\n",
    "        Output:\n",
    "            (return value in forward) a tensor of shape (batch_size, out_size)\n",
    "        '''\n",
    "        super(AuViSubNet, self).__init__()\n",
    "        self.rnn = nn.LSTM(in_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_1 = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        '''\n",
    "        x: (batch_size, sequence_len, in_size)\n",
    "        '''\n",
    "        packed_sequence = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, final_states = self.rnn(packed_sequence)\n",
    "        h = self.dropout(final_states[0].squeeze())\n",
    "        y_1 = self.linear_1(h)\n",
    "        return y_1\n",
    "    \n",
    "    \n",
    "class SELF_MM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SELF_MM, self).__init__()\n",
    "        print('create SELF_MM model')\n",
    "        \n",
    "        self.audio_in = ACOUSTIC_DIM\n",
    "        self.video_in = VISUAL_DIM\n",
    "        self.text_in = TEXT_DIM\n",
    "        self.audio_hidden = 8\n",
    "        self.video_hidden = 64\n",
    "        self.text_hidden = 64\n",
    "        self.text_out = 768\n",
    "        self.post_text_dim = 32\n",
    "        self.audio_out = 8\n",
    "        self.post_audio_dim = 8 \n",
    "        self.video_out = 32\n",
    "        self.post_video_dim = 32 \n",
    "        self.post_fusion_dim = 32\n",
    "        self.audio_prob, self.video_prob, self.text_prob, self.post_fusion_prob = 0.2, 0.2, 0.2, 0.2\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('./bert-base-uncased/')\n",
    "        self.audio_model = AuViSubNet(self.audio_in, self.audio_hidden, self.audio_out, dropout = self.audio_prob)\n",
    "        self.video_model = AuViSubNet(self.video_in, self.video_hidden, self.video_out, dropout = self.video_prob)\n",
    "\n",
    "        self.post_fusion_dropout = nn.Dropout(p=self.post_fusion_prob)\n",
    "        self.post_fusion_layer_1 = nn.Linear(self.text_out + self.video_out + self.audio_out,\n",
    "                                             self.post_fusion_dim)\n",
    "        self.post_fusion_layer_2 = nn.Linear(self.post_fusion_dim, self.post_fusion_dim)\n",
    "        self.post_fusion_layer_3 = nn.Linear(self.post_fusion_dim, 1)\n",
    "        \n",
    "\n",
    "        # the classify layer for text\n",
    "        self.post_text_dropout = nn.Dropout(p=self.text_prob)\n",
    "        self.post_text_layer_1 = nn.Linear(self.text_out, self.post_text_dim)\n",
    "        self.post_text_layer_2 = nn.Linear(self.post_text_dim, self.post_text_dim)\n",
    "        self.post_text_layer_3 = nn.Linear(self.post_text_dim, 1)\n",
    "        \n",
    "        \n",
    "        # the classify layer for audio\n",
    "        self.post_audio_dropout = nn.Dropout(p=self.audio_prob)\n",
    "        self.post_audio_layer_1 = nn.Linear(self.audio_out, self.post_audio_dim)\n",
    "        self.post_audio_layer_2 = nn.Linear(self.post_audio_dim, self.post_audio_dim)\n",
    "        self.post_audio_layer_3 = nn.Linear(self.post_audio_dim, 1)\n",
    "\n",
    "        # the classify layer for video\n",
    "\n",
    "        self.post_video_dropout = nn.Dropout(p=self.video_prob)\n",
    "        self.post_video_layer_1 = nn.Linear(self.video_out, self.post_video_dim)\n",
    "        self.post_video_layer_2 = nn.Linear(self.post_video_dim, self.post_video_dim)\n",
    "        self.post_video_layer_3 = nn.Linear(self.post_video_dim, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bert_indices = inputs['bert_indices']\n",
    "        box_vit = inputs['video_features_p']\n",
    "        bert_text_len = torch.sum(bert_indices != 0, dim = -1)\n",
    "        # 2,24, audio_feature key 2 33 33 , 2,10 768 \n",
    "        visual = box_vit\n",
    "        acoustic = inputs['audio_feature']\n",
    "        self.align_subnet = AlignSubNet(bert_indices.size(1))\n",
    "        bert_indices, acoustic, visual= self.align_subnet(bert_indices,acoustic,visual)\n",
    "        acoustic = acoustic.float()\n",
    "        outputs = self.bert(bert_indices)\n",
    "        encoder_layer = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        text_x = encoder_layer\n",
    "        audio_x = acoustic\n",
    "        video_x = visual\n",
    "        \n",
    "        text = pooled_output\n",
    "        audio = self.audio_model(audio_x, bert_text_len)\n",
    "        video = self.video_model(video_x, bert_text_len)\n",
    "        \n",
    "        # fusion\n",
    "        fusion_h = torch.cat([text, audio, video], dim=-1)\n",
    "        fusion_h = self.post_fusion_dropout(fusion_h)\n",
    "        fusion_h = F.relu(self.post_fusion_layer_1(fusion_h), inplace=False)\n",
    "        # # text\n",
    "        text_h = self.post_text_dropout(text)\n",
    "        text_h = F.relu(self.post_text_layer_1(text_h), inplace=False)\n",
    "        # audio\n",
    "        audio_h = self.post_audio_dropout(audio)\n",
    "        audio_h = F.relu(self.post_audio_layer_1(audio_h), inplace=False)\n",
    "        # vision\n",
    "        video_h = self.post_video_dropout(video)\n",
    "        video_h = F.relu(self.post_video_layer_1(video_h), inplace=False)\n",
    "\n",
    "        # classifier-fusion\n",
    "        x_f = F.relu(self.post_fusion_layer_2(fusion_h), inplace=False)\n",
    "        output_fusion = self.post_fusion_layer_3(x_f)\n",
    "\n",
    "        # classifier-text\n",
    "        x_t = F.relu(self.post_text_layer_2(text_h), inplace=False)\n",
    "        output_text = self.post_text_layer_3(x_t)\n",
    "\n",
    "        # classifier-audio\n",
    "        x_a = F.relu(self.post_audio_layer_2(audio_h), inplace=False)\n",
    "        output_audio = self.post_audio_layer_3(x_a)\n",
    "\n",
    "        # classifier-vision\n",
    "        x_v = F.relu(self.post_video_layer_2(video_h), inplace=False)\n",
    "        output_video = self.post_video_layer_3(x_v)\n",
    "        \n",
    "        res = {\n",
    "            'M': output_fusion, \n",
    "            'T': output_text,\n",
    "            'A': output_audio,\n",
    "            'V': output_video,\n",
    "            'Feature_t': text_h,\n",
    "            'Feature_a': audio_h,\n",
    "            'Feature_v': video_h,\n",
    "            'Feature_f': fusion_h,\n",
    "        }\n",
    "        return res\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15485742-17a5-4dfb-b0fd-d581ba2157fe",
   "metadata": {},
   "source": [
    "# 初始模型参数，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f39f1fe-d452-4909-a7e8-01971bb24366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_cmgcn_improve_params(cmgcni_model):\n",
    "    for child in cmgcni_model.children():\n",
    "        # print(type(child) != BertModel)\n",
    "        if type(child) != MAG_BertModel:\n",
    "            for p in child.parameters():\n",
    "                if p.requires_grad :\n",
    "                    if len(p.shape) > 1:\n",
    "                        torch.nn.init.xavier_uniform_(p)\n",
    "                    else:\n",
    "                        import math\n",
    "                        stdv = 1.0 / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "    print('init_cmgcn_improve_params()')\n",
    "\n",
    "def init_cmgcn_params(cmgcn_model):\n",
    "    for child in cmgcn_model.children():\n",
    "        # print(type(child) != BertModel)\n",
    "        if type(child) != BertModel:\n",
    "            for p in child.parameters():\n",
    "                if p.requires_grad :\n",
    "                    if len(p.shape) > 1:\n",
    "                        torch.nn.init.xavier_uniform_(p)\n",
    "                    else:\n",
    "                        import math\n",
    "                        stdv = 1.0 / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "    print('init_cmgcn_params()')\n",
    "\n",
    "\n",
    "def init_magbert_params(magbert_forseqcls):\n",
    "    for child in magbert_forseqcls.children():\n",
    "        if type(child) != MAG_BertModel:\n",
    "            for p in child.parameters():\n",
    "                if p.requires_grad :\n",
    "                    if len(p.shape) > 1:\n",
    "                        torch.nn.init.xavier_uniform_(p)\n",
    "                    else:\n",
    "                        import math\n",
    "                        stdv = 1.0 / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "    print('init_magbert_params()')\n",
    "\n",
    "def init_ef_lstm_params(ef_lstm_model):\n",
    "    for child in ef_lstm_model.children():\n",
    "        if type(child) != BertModel:\n",
    "            for p in child.parameters():\n",
    "                if p.requires_grad :\n",
    "                    if len(p.shape) > 1:\n",
    "                        torch.nn.init.xavier_uniform_(p)\n",
    "                    else:\n",
    "                        import math\n",
    "                        stdv = 1.0 / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "    print('init_ef_lstm_params()')\n",
    "    \n",
    "def init_lf_dnn_params(lf_dnn_model):\n",
    "    for child in lf_dnn_model.children():\n",
    "        if type(child) != BertModel:\n",
    "            for p in child.parameters():\n",
    "                if p.requires_grad :\n",
    "                    if len(p.shape) > 1:\n",
    "                        torch.nn.init.xavier_uniform_(p)\n",
    "                    else:\n",
    "                        import math\n",
    "                        stdv = 1.0 / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "    print('init_lf_dnn_params()')\n",
    "    \n",
    "\n",
    "\n",
    "def init_self_mm_params(self_mm_model):\n",
    "    for child in self_mm_model.children():\n",
    "        if type(child) != BertModel:\n",
    "            for p in child.parameters():\n",
    "                if p.requires_grad :\n",
    "                    if len(p.shape) > 1:\n",
    "                        torch.nn.init.xavier_uniform_(p)\n",
    "                    else:\n",
    "                        import math\n",
    "                        stdv = 1.0 / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "    print('init_self_mm_params()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a9bb1-552e-4e66-bd51-13b91de0d4a6",
   "metadata": {},
   "source": [
    "# 准备optimizer函数，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c88eee06-4505-4f16-966d-bae7400f9a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cmgcn_improve_optimizer(cmgcni_model):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params':cmgcni_model.mag_bert.parameters(),'lr':2e-5},\n",
    "        {'params':cmgcni_model.text_lstm.parameters(),},\n",
    "        {'params':cmgcni_model.vit_fc.parameters(),},\n",
    "        {'params':cmgcni_model.gc1.parameters(),},\n",
    "        {'params':cmgcni_model.gc2.parameters(),},\n",
    "        {'params':cmgcni_model.fc.parameters(),},\n",
    "    ],lr=0.001,weight_decay=1e-5)\n",
    "    # optimizer = torch.optim.Adam(cmgcn_model.parameters(),lr=1e-3,weight_decay=1e-5)\n",
    "    return optimizer\n",
    "\n",
    "def get_cmgcn_optimizer(cmgcn_model):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params':cmgcn_model.bert.parameters(),'lr':2e-5},\n",
    "        {'params':cmgcn_model.text_lstm.parameters(),},\n",
    "        {'params':cmgcn_model.vit_fc.parameters(),},\n",
    "        {'params':cmgcn_model.gc1.parameters(),},\n",
    "        {'params':cmgcn_model.gc2.parameters(),},\n",
    "        {'params':cmgcn_model.fc.parameters(),},\n",
    "    ],lr=0.001,weight_decay=1e-5)\n",
    "    # optimizer = torch.optim.Adam(cmgcn_model.parameters(),lr=1e-3,weight_decay=1e-5)\n",
    "    return optimizer\n",
    "\n",
    "def get_magbert_optimizer(magbert_forseqcls):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params':magbert_forseqcls.mag_bert.parameters(),'lr':1e-5},\n",
    "        {'params':magbert_forseqcls.classifier.parameters(),},\n",
    "        {'params':magbert_forseqcls.dropout.parameters(),},\n",
    "    ],lr=0.001,weight_decay=1e-5)\n",
    "    return optimizer\n",
    "\n",
    "def get_ef_lstm_optimizer(ef_lstm_model):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params':ef_lstm_model.bert.parameters(),'lr':1e-5},\n",
    "        {'params':ef_lstm_model.norm.parameters(),},\n",
    "        {'params':ef_lstm_model.lstm.parameters(),},\n",
    "        {'params':ef_lstm_model.dropout.parameters(),},\n",
    "        {'params':ef_lstm_model.linear.parameters(),},\n",
    "        {'params':ef_lstm_model.out.parameters(),},\n",
    "    ],lr=0.001,weight_decay=1e-5)\n",
    "    return optimizer\n",
    "\n",
    "def get_lf_dnn_optimizer(lf_dnn_model):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params':lf_dnn_model.bert.parameters(),'lr':1e-5},\n",
    "        {'params':lf_dnn_model.audio_subnet.parameters(),},\n",
    "        {'params':lf_dnn_model.video_subnet.parameters(),},\n",
    "        {'params':lf_dnn_model.text_subnet.parameters(),},\n",
    "        {'params':lf_dnn_model.post_fusion_dropout.parameters(),},\n",
    "        {'params':lf_dnn_model.post_fusion_layer_1.parameters(),},\n",
    "        {'params':lf_dnn_model.post_fusion_layer_2.parameters(),},\n",
    "        {'params':lf_dnn_model.post_fusion_layer_3.parameters(),},\n",
    "    ],lr=0.001,weight_decay=1e-5)\n",
    "    return optimizer\n",
    "\n",
    "def get_self_mm_optimizer(self_mm_model):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params':self_mm_model.bert.parameters(),'lr':1e-5},\n",
    "        {'params':self_mm_model.audio_model.parameters(),},\n",
    "        {'params':self_mm_model.video_model.parameters(),},\n",
    "        \n",
    "        {'params':self_mm_model.post_fusion_dropout.parameters(),},\n",
    "        {'params':self_mm_model.post_fusion_layer_1.parameters(),},\n",
    "        {'params':self_mm_model.post_fusion_layer_2.parameters(),},\n",
    "        {'params':self_mm_model.post_fusion_layer_3.parameters(),},\n",
    "        \n",
    "        {'params':self_mm_model.post_text_dropout.parameters(),},\n",
    "        {'params':self_mm_model.post_text_layer_1.parameters(),},\n",
    "        {'params':self_mm_model.post_text_layer_2.parameters(),},\n",
    "        {'params':self_mm_model.post_text_layer_3.parameters(),},\n",
    "        \n",
    "        {'params':self_mm_model.post_audio_dropout.parameters(),},\n",
    "        {'params':self_mm_model.post_audio_layer_1.parameters(),},\n",
    "        {'params':self_mm_model.post_audio_layer_2.parameters(),},\n",
    "        {'params':self_mm_model.post_audio_layer_3.parameters(),},\n",
    "        \n",
    "        {'params':self_mm_model.post_video_dropout.parameters(),},\n",
    "        {'params':self_mm_model.post_video_layer_1.parameters(),},\n",
    "        {'params':self_mm_model.post_video_layer_2.parameters(),},\n",
    "        {'params':self_mm_model.post_video_layer_3.parameters(),},\n",
    "        \n",
    "    ],lr=0.001,weight_decay=1e-5)\n",
    "    return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5d176-3029-469a-b87a-7da428716be4",
   "metadata": {},
   "source": [
    "# 调试模型的形状函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7913973-f64d-4dfb-8fcd-bd799cbfed73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def debug_model(model):\n",
    "    d = MUStartDataset('valid')\n",
    "    dl = DataLoader(d, batch_size=2, num_workers=0, shuffle=False)\n",
    "    batch = iter(dl).next()\n",
    "    batch.keys()\n",
    "    inputs ={}\n",
    "    for key in batch.keys():\n",
    "        inputs[key] = batch[key].to(device)\n",
    "    outputs = model(inputs)\n",
    "    print('debug_model: ',type(model),outputs.shape)\n",
    "    \n",
    "\n",
    "def debug_self_mm_model(model):\n",
    "    d = MUStartDataset('train')\n",
    "    dl = DataLoader(d, batch_size=2, num_workers=0, shuffle=False)\n",
    "    batch = iter(dl).next()\n",
    "    batch.keys()\n",
    "    inputs ={}\n",
    "    for key in batch.keys():\n",
    "        inputs[key] = batch[key].to(device)\n",
    "    outputs = model(inputs)\n",
    "    print('debug_model: ',type(model))\n",
    "    print([(k,v.shape) for k,v in outputs.items()])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426573c-490f-4bb2-945a-c79fb389bd05",
   "metadata": {},
   "source": [
    "# train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0de29f4a-50c5-4960-84cb-c23e3eb08bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_acc_f1(data_loader,model):\n",
    "    n_correct, n_total = 0, 0\n",
    "    targets_all, outputs_all = None, None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i_batch,batch in enumerate(data_loader):\n",
    "            inputs ={}\n",
    "            for key in batch.keys():\n",
    "                inputs[key] = batch[key].to(device)\n",
    "            outputs = model(inputs)\n",
    "            targets = batch['labels'].to(device)\n",
    "            \n",
    "            n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "            n_total += len(outputs)\n",
    "            \n",
    "            if targets_all is None:\n",
    "                targets_all = targets\n",
    "                outputs_all = outputs\n",
    "            else:\n",
    "                targets_all = torch.cat((targets_all,targets), dim=0)\n",
    "                outputs_all = torch.cat((outputs_all,outputs), dim=0)\n",
    "    \n",
    "\n",
    "    \n",
    "    acc = n_correct / n_total\n",
    "    f1 = metrics.f1_score(targets_all.cpu(), torch.argmax(outputs_all,-1).cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "    precision = metrics.precision_score(targets_all.cpu(), torch.argmax(outputs_all,-1).cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "    recall = metrics.recall_score(targets_all.cpu(), torch.argmax(outputs_all,-1).cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "\n",
    "    return acc,f1,precision,recall\n",
    "\n",
    "def evaluate_self_mm_acc_f1(data_loader,model):\n",
    "    n_correct, n_total = 0, 0\n",
    "    targets_all, outputs_all = None, None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i_batch,batch in enumerate(data_loader):\n",
    "            inputs ={}\n",
    "            for key in batch.keys():\n",
    "                inputs[key] = batch[key].to(device)\n",
    "            outputs = model(inputs)\n",
    "            targets = batch['labels'].to(device)\n",
    "            output_label = outputs['M'].view(-1)\n",
    "            output_label[output_label>=0.5] = 1.0\n",
    "            output_label[output_label<0.5] = 0.0\n",
    "            \n",
    "            n_correct += (output_label == targets).sum().item()\n",
    "            n_total += len(outputs['M'])\n",
    "            \n",
    "            if targets_all is None:\n",
    "                targets_all = targets\n",
    "                outputs_all = outputs['M']\n",
    "            else:\n",
    "                targets_all = torch.cat((targets_all,targets), dim=0)\n",
    "                outputs_all = torch.cat((outputs_all,outputs['M']), dim=0)\n",
    "    \n",
    "    \n",
    "    acc = n_correct / n_total\n",
    "    f1 = metrics.f1_score(targets_all.cpu(), outputs_all.cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "    precision = metrics.precision_score(targets_all.cpu(), outputs_all.cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "    recall = metrics.recall_score(targets_all.cpu(), outputs_all.cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "\n",
    "    return acc,f1,precision,recall\n",
    "\n",
    "\n",
    "\n",
    "def train(model,optimizer,model_save_path):\n",
    "    max_val_acc , max_val_f1, max_val_epoch, global_step = 0, 0, 0, 0\n",
    "    for i_epoch in range(num_epoch):\n",
    "        print('i_epoch:', i_epoch)\n",
    "        n_correct, n_total, loss_total = 0, 0, 0\n",
    "        for i_batch,batch in enumerate(train_dataloader):\n",
    "            global_step += 1\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            inputs ={}\n",
    "            for key in batch.keys():\n",
    "                inputs[key] = batch[key].to(device)\n",
    "            outputs = model(inputs)\n",
    "            targets = batch['labels'].to(device)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "            n_total += len(outputs)\n",
    "            loss_total += loss.item() * len(outputs)\n",
    "\n",
    "            train_acc = n_correct / n_total\n",
    "            train_loss = loss_total / n_total\n",
    "\n",
    "            if global_step % 1 == 0:\n",
    "                val_acc, val_f1, val_precision, val_recall = evaluate_acc_f1(valid_dataloader,model)\n",
    "                if val_acc >= max_val_acc:\n",
    "                    max_val_f1 = val_f1\n",
    "                    max_val_acc = val_acc\n",
    "                    max_val_epoch = i_epoch\n",
    "                    torch.save(model.state_dict(),model_save_path)\n",
    "                    print('save the model to {}'.format(model_save_path))\n",
    "\n",
    "        if i_epoch - max_val_epoch >= 0:\n",
    "            print('early stop')\n",
    "            break\n",
    "\n",
    "        break\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    test_acc, test_f1,test_precision,test_recall = evaluate_acc_f1(test_dataloader,model)\n",
    "    print('test_acc:', test_acc)\n",
    "    print('test_f1:', test_f1)\n",
    "    print('test_precision', test_precision)\n",
    "    print('test_recall', test_recall)\n",
    "\n",
    "\n",
    "def train_self_mm(model,optimizer,model_save_path):\n",
    "    # 多任务self逻辑\n",
    "    train_samples = len(train_dataset)\n",
    "    label_map = {\n",
    "        'fusion': torch.zeros(train_samples, requires_grad=False).to(device),\n",
    "        'text': torch.zeros(train_samples, requires_grad=False).to(device),\n",
    "        'audio': torch.zeros(train_samples, requires_grad=False).to(device),\n",
    "        'vision': torch.zeros(train_samples, requires_grad=False).to(device)\n",
    "    }\n",
    "    # init labels\n",
    "    for batch_data in train_dataloader:\n",
    "        labels_m = batch_data['labels'].float()\n",
    "        label_map['fusion'] = labels_m\n",
    "        label_map['text'] = labels_m\n",
    "        label_map['audion'] = labels_m\n",
    "        label_map['vision'] = labels_m\n",
    "        \n",
    "    max_val_acc , max_val_f1, max_val_epoch, global_step = 0, 0, 0, 0\n",
    "    for i_epoch in range(num_epoch):\n",
    "        epochs = i_epoch + 1             # for same with self_mm original code \n",
    "        print('epochs:', epochs)\n",
    "        y_pred = {'M': [], 'T': [], 'A': [], 'V': []}\n",
    "        y_true = {'M': [], 'T': [], 'A': [], 'V': []}\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        n_correct, n_total, loss_total = 0, 0, 0\n",
    "        for i_batch,batch in enumerate(train_dataloader):\n",
    "            global_step += 1\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            inputs ={}\n",
    "            for key in batch.keys():\n",
    "                inputs[key] = batch[key].to(device)\n",
    "                \n",
    "            indexes = batch['index'].view(-1)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            tasks = 'MTAV'\n",
    "            name_map = {\n",
    "                'M': 'fusion',\n",
    "                'T': 'text',\n",
    "                'A': 'audio',\n",
    "                'V': 'vision'\n",
    "            }\n",
    "            for m in tasks:\n",
    "                y_pred[m].append(outputs[m].cpu())\n",
    "                y_true[m].append(label_map[name_map[m]][indexes].cpu())\n",
    "                \n",
    "            loss = 0.0\n",
    "            def weighted_loss(y_pred,y_true,indexes=None, mode = 'fusion'):\n",
    "                if mode == 'fusion':\n",
    "                    weighted = torch.ones_like(y_pred)\n",
    "                else:\n",
    "                    weighted = torch.tanh(torch.abs(label_map[mode][indexes] - label_map['fusion'][indexes]))\n",
    "                loss = torch.mean(weighted * torch.abs(y_pred - y_true))\n",
    "                return loss\n",
    "                \n",
    "            for m in tasks:\n",
    "                loss += weighted_loss(outputs[m],label_map[name_map[m]][indexes],indexes = indexes, mode = name_map[m])\n",
    "            \n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            # update features \n",
    "            f_fusion = outputs['Feature_f'].detach()\n",
    "            f_text = outputs['Feature_t'].detach()\n",
    "            f_audio = outputs['Feature_a'].detach()\n",
    "            f_vision = outputs['Feature_v'].detach()\n",
    "            def update_labels(f_fusion, f_text, f_audio, f_vision, cur_epoches, indexes, outputs):\n",
    "                MIN = 1e-8\n",
    "                def update_single_label(f_single, mode):\n",
    "                    d_sp = torch.norm(f_single - center_map[mode]['pos'], dim=-1) \n",
    "                    d_sn = torch.norm(f_single - center_map[mode]['neg'], dim=-1) \n",
    "                    delta_s = (d_sn - d_sp) / (d_sp + MIN)\n",
    "                    alpha = delta_s / (delta_f + MIN)\n",
    "                    \n",
    "                    new_labels = 0.5 * alpha * label_map['fusion'][indexes] + \\\n",
    "                        0.5 * (label_map['fusion'][indexes] + delta_s - delta_f)\n",
    "                    new_labels = torch.clamp(new_labels, min=0.0, max=1.0)\n",
    "                    \n",
    "                    n = cur_epoches\n",
    "                    label_map[mode][indexes] = (n - 1) / (n + 1) * label_map[mode][indexes] \\\n",
    "                      + 2 / (n + 1) * new_labels\n",
    "                    \n",
    "                d_fp = torch.norm(f_fusion - center_map['fusion']['pos'], dim=-1)\n",
    "                d_fn = torch.norm(f_fusion - center_map['fusion']['neg'], dim=-1) \n",
    "                delta_f = (d_fn - d_fp) / (d_fp + MIN)\n",
    "                \n",
    "                update_single_label(f_text, mode='text')\n",
    "                update_single_label(f_audio, mode='audio')\n",
    "                update_single_label(f_vision, mode='vision')\n",
    "            \n",
    "            if epochs > 1:\n",
    "                update_labels(f_fusion, f_text, f_audio, f_vision, epochs, indexes, outputs)\n",
    "            \n",
    "            print(211, label_map)\n",
    "            post_fusion_dim = 32\n",
    "            post_text_dim = 32\n",
    "            post_audio_dim = 8\n",
    "            post_video_dim = 32\n",
    "            feature_map = {\n",
    "                'fusion': torch.zeros(train_samples, post_fusion_dim, requires_grad=False).to(device),\n",
    "                'text': torch.zeros(train_samples, post_text_dim, requires_grad=False).to(device),\n",
    "                'audio': torch.zeros(train_samples, post_audio_dim, requires_grad=False).to(device),\n",
    "                'vision': torch.zeros(train_samples, post_video_dim, requires_grad=False).to(device),\n",
    "            }\n",
    "            def update_features(f_fusion, f_text, f_audio, f_vision, indexes):\n",
    "                feature_map['fusion'][indexes] = f_fusion\n",
    "                feature_map['text'][indexes] = f_text\n",
    "                feature_map['audio'][indexes] = f_audio\n",
    "                feature_map['vision'][indexes] = f_vision\n",
    "            \n",
    "            center_map = {\n",
    "                'fusion': {\n",
    "                    'pos': torch.zeros(post_fusion_dim, requires_grad=False).to(device),\n",
    "                    'neg': torch.zeros(post_fusion_dim, requires_grad=False).to(device),\n",
    "                },\n",
    "                'text': {\n",
    "                    'pos': torch.zeros(post_text_dim, requires_grad=False).to(device),\n",
    "                    'neg': torch.zeros(post_text_dim, requires_grad=False).to(device),\n",
    "                },\n",
    "                'audio': {\n",
    "                    'pos': torch.zeros(post_audio_dim, requires_grad=False).to(device),\n",
    "                    'neg': torch.zeros(post_audio_dim, requires_grad=False).to(device),\n",
    "                },\n",
    "                'vision': {\n",
    "                    'pos': torch.zeros(post_video_dim, requires_grad=False).to(device),\n",
    "                    'neg': torch.zeros(post_video_dim, requires_grad=False).to(device),\n",
    "                }\n",
    "            }\n",
    "            def update_centers():\n",
    "                def update_single_center(mode):\n",
    "                    neg_indexes = label_map[mode] <= 0.5   # [0 1] label \n",
    "                    pos_indexes = label_map[mode] > 0.5\n",
    "                    if torch.any(pos_indexes):\n",
    "                        center_map[mode]['pos'] = torch.mean(feature_map[mode][pos_indexes], dim=0)\n",
    "                    if torch.any(neg_indexes):\n",
    "                        center_map[mode]['neg'] = torch.mean(feature_map[mode][neg_indexes], dim=0)\n",
    "                    # 如果样本只有正例 ，则负例会出现nan的情况\n",
    "                    \n",
    "                update_single_center(mode='fusion')\n",
    "                update_single_center(mode='text')\n",
    "                update_single_center(mode='audio')\n",
    "                update_single_center(mode='vision')\n",
    "                \n",
    "                \n",
    "            update_features(f_fusion, f_text, f_audio, f_vision, indexes)\n",
    "            update_centers()\n",
    "            optimizer.step()\n",
    "            \n",
    "            targets = batch['labels'].to(device)\n",
    "            # outputs['M'] >= 0.5 1 \n",
    "            # outputs['M'] < 0.5 0 \n",
    "            output_label = outputs['M'].view(-1)\n",
    "            output_label[output_label>=0.5] = 1.0\n",
    "            output_label[output_label<0.5] = 0.0\n",
    "\n",
    "            n_correct += (output_label == targets).sum().item()\n",
    "            n_total += len(outputs['M'])\n",
    "            loss_total += train_loss * len(outputs['M'])\n",
    "\n",
    "            train_acc = n_correct / n_total\n",
    "            train_loss = loss_total / n_total\n",
    "\n",
    "            if global_step % 1 == 0:\n",
    "                val_acc, val_f1, val_precision, val_recall = evaluate_self_mm_acc_f1(valid_dataloader,model)\n",
    "                if val_acc >= max_val_acc:\n",
    "                    max_val_f1 = val_f1\n",
    "                    max_val_acc = val_acc\n",
    "                    max_val_epoch = i_epoch\n",
    "                    torch.save(model.state_dict(),model_save_path)\n",
    "                    print('save the model to {}'.format(model_save_path))\n",
    "\n",
    "        if i_epoch - max_val_epoch > 0:\n",
    "            print('early stop')\n",
    "            break\n",
    "\n",
    "        # break\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    test_acc, test_f1,test_precision,test_recall = evaluate_self_mm_acc_f1(test_dataloader,model)\n",
    "    print('test_acc:', test_acc)\n",
    "    print('test_f1:', test_f1)\n",
    "    print('test_precision', test_precision)\n",
    "    print('test_recall', test_recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219186dc-b6cb-4ecf-a03d-712474758414",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 准备训练不同模型的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb2057e-41e0-46d8-9f76-29259308d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model \n",
    "def train_cmgcni_model():\n",
    "    cmgcni_model = CMGCNI(multimodal_config = multimodal_config ).to(device)\n",
    "    init_cmgcn_improve_params(cmgcni_model) \n",
    "    cmgcni_optimizer = get_cmgcn_improve_optimizer(cmgcni_model)\n",
    "    debug_model(cmgcni_model)\n",
    "    cmgcni_model_path = '/tmp/cmgcni_model.pth'\n",
    "\n",
    "    model = cmgcni_model\n",
    "    optimizer = cmgcni_optimizer\n",
    "    model_save_path = cmgcni_model_path\n",
    "    print('start train:' + '-'*10)\n",
    "    train(model,optimizer,model_save_path)\n",
    "\n",
    "# prepare model \n",
    "def train_cmgcn_model():\n",
    "    cmgcn_model = CMGCN().to(device)\n",
    "    init_cmgcn_params(cmgcn_model) \n",
    "    cmgcn_optimizer = get_cmgcn_optimizer(cmgcn_model)\n",
    "    debug_model(cmgcn_model)\n",
    "    cmgcn_model_path = '/tmp/cmgcn_model.pth'\n",
    "\n",
    "    model = cmgcn_model\n",
    "    optimizer = cmgcn_optimizer\n",
    "    model_save_path = cmgcn_model_path\n",
    "    print('start train:' + '-'*10)\n",
    "    train(model,optimizer,model_save_path)\n",
    "\n",
    "def train_magbert_forseqcls():\n",
    "    magbert_forseqcls = MagBertForSequenceClassification(multimodal_config = multimodal_config ).to(device)\n",
    "    init_magbert_params(magbert_forseqcls)\n",
    "    magbert_optimizer = get_magbert_optimizer(magbert_forseqcls)\n",
    "    debug_model(magbert_forseqcls)\n",
    "    magbert_model_path = '/tmp/magbert_model.pth'\n",
    "\n",
    "    model = magbert_forseqcls\n",
    "    optimizer = magbert_optimizer\n",
    "    model_save_path = magbert_model_path\n",
    "    print('start train:' + '-'*10)\n",
    "    train(model,optimizer,model_save_path)\n",
    "\n",
    "def train_ef_lstm_model():\n",
    "    ef_lstm_model = EF_LSTM().to(device)\n",
    "    init_ef_lstm_params(ef_lstm_model)\n",
    "    ef_lstm_optimizer = get_ef_lstm_optimizer(ef_lstm_model)\n",
    "    debug_model(ef_lstm_model)\n",
    "    ef_lstm_model_path = '/tmp/ef_lstm_model.pth'\n",
    "\n",
    "    model = ef_lstm_model\n",
    "    optimizer = ef_lstm_optimizer\n",
    "    model_save_path = ef_lstm_model_path\n",
    "    print('start train:' + '-'*10)\n",
    "    train(model,optimizer,model_save_path)\n",
    "    \n",
    "def train_lf_dnn_model():\n",
    "    lf_dnn_model = LF_DNN().to(device)\n",
    "    init_lf_dnn_params(lf_dnn_model)\n",
    "    lf_dnn_optimizer = get_lf_dnn_optimizer(lf_dnn_model)\n",
    "    debug_model(lf_dnn_model)\n",
    "    lf_dnn_model_path = '/tmp/lf_dnn_model.pth'\n",
    "\n",
    "    model = lf_dnn_model\n",
    "    optimizer = lf_dnn_optimizer\n",
    "    model_save_path = lf_dnn_model_path\n",
    "    print('start train:' + '-'*10)\n",
    "    train(model,optimizer,model_save_path)\n",
    "    \n",
    "def train_self_mm_model():\n",
    "    self_mm_model = SELF_MM().to(device)\n",
    "    init_self_mm_params(self_mm_model)\n",
    "    self_mm_optimizer = get_self_mm_optimizer(self_mm_model)\n",
    "    debug_self_mm_model(self_mm_model)\n",
    "    self_mm_model_path = '/tmp/self_mm_model.pth'\n",
    "\n",
    "    model = self_mm_model\n",
    "    optimizer = self_mm_optimizer\n",
    "    model_save_path = self_mm_model_path\n",
    "    print('start train:' + '-'*10)\n",
    "    train_self_mm(model,optimizer,model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84085948-5fa9-4f97-9aba-b3004204a11d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207144b9-066e-4235-884d-0750c8fc99f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11632\\2094403463.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda:0'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'use device: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "beta_shift = 1.0 \n",
    "dropout_prob = 0.5 \n",
    "multimodal_config = MultimodalConfig(\n",
    "    beta_shift=beta_shift, dropout_prob=dropout_prob\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0') if use_cuda else torch.device('cpu')\n",
    "logger.info('use device: {}'.format(device))\n",
    "BATCH_SIZE = 2 \n",
    "num_epoch = 2\n",
    "\n",
    "d = MUStartDataset('valid')\n",
    "dl = DataLoader(d, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "batch_sample = iter(dl).next()\n",
    "\n",
    "ACOUSTIC_DIM = batch_sample['audio_feature'].size(2)\n",
    "VISUAL_DIM = batch_sample['video_features_p'].size(2)\n",
    "TEXT_DIM = 768\n",
    "TEXT_SEQ_LEN = batch_sample['bert_indices'].size(1)\n",
    "\n",
    "\n",
    "train_dataset = MUStartDataset(mode='train')\n",
    "valid_dataset = MUStartDataset(mode='valid')\n",
    "test_dataset = MUStartDataset(mode='test')\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,num_workers=0,shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,num_workers=0,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,num_workers=0,shuffle=False)\n",
    "\n",
    "\n",
    "train_model_name = 'cmgcni'\n",
    "\n",
    "if train_model_name == 'EF_LSTM':\n",
    "    train_ef_lstm_model()\n",
    "elif train_model_name == 'lf_dnn':\n",
    "    train_lf_dnn_model()\n",
    "elif train_model_name == 'self_mm':\n",
    "    train_self_mm_model()\n",
    "elif train_model_name == 'mag_bert':\n",
    "    train_magbert_forseqcls()\n",
    "elif train_model_name == 'cmgcn':\n",
    "    train_cmgcn_model()\n",
    "elif train_model_name == 'cmgcni':\n",
    "    train_cmgcni_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
