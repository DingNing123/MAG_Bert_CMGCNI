{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76381e87-9146-4755-bc3c-9441994d1566",
   "metadata": {},
   "source": [
    "# Coat 模型\n",
    "\n",
    "https://github.com/DingNing123/CoaT\n",
    "\n",
    "\n",
    "9.Co-Scale Conv-Attentional Image Transformers.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb5dd00-8827-4d03-9b4b-4ac530d33e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InterpolationMode.BICUBIC: 'bicubic'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "InterpolationMode.BICUBIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a7a4bac-22df-44c7-b03d-e66f541d8b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "class args:\n",
    "    device = 'cpu'\n",
    "    input_size = 224\n",
    "    color_jitter = 0.4\n",
    "    aa ='rand-m9-mstd0.5-inc1'\n",
    "    train_interpolation = InterpolationMode.BICUBIC\n",
    "    reprob = 0.25\n",
    "    remode = 'pixel'\n",
    "    recount = 1\n",
    "    data_path = './data/ImageNet'\n",
    "    data_set = 'IMNET'\n",
    "    batch_size = 2\n",
    "    num_workers = 0 \n",
    "    pin_mem = False\n",
    "    model = 'coat_lite_tiny'\n",
    "    nb_classes = 1000\n",
    "    drop = 0.0\n",
    "    drop_path = 0.0\n",
    "    drop_block = None\n",
    "    model_kwargs = '{}'\n",
    "    lr = 5e-4\n",
    "    batch_size = 2\n",
    "    \n",
    "    \n",
    "device = torch.device(args.device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e1c26f-e371-4178-af52-ff09b567ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "def build_transform(is_train, args):\n",
    "    resize_im = args.input_size > 32\n",
    "    if is_train:\n",
    "        # this should always dispatch to transforms_imagenet_train\n",
    "        transform = create_transform(\n",
    "            input_size=args.input_size,\n",
    "            is_training=True,\n",
    "            color_jitter=args.color_jitter,\n",
    "            auto_augment=args.aa,\n",
    "            interpolation=args.train_interpolation,\n",
    "            re_prob=args.reprob,\n",
    "            re_mode=args.remode,\n",
    "            re_count=args.recount,\n",
    "        ) \n",
    "        if not resize_im:\n",
    "            # replace RandomResizedCropAndInterpolation with\n",
    "            # RandomCrop\n",
    "            transform.transforms[0] = transforms.RandomCrop(\n",
    "                args.input_size, padding=4)\n",
    "        return transform\n",
    "\n",
    "    t = []  # Test-time transformations.\n",
    "    if resize_im:\n",
    "        size = int((256 / 224) * args.input_size)\n",
    "        t.append(\n",
    "            transforms.Resize(size, interpolation=3),  # to maintain same ratio w.r.t. 224 images\n",
    "        ) \n",
    "        t.append(transforms.CenterCrop(args.input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n",
    "    return transforms.Compose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a51c073-2cd4-40b9-9298-922b3972ebae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    <timm.data.auto_augment.RandAugment object at 0x13558c208>\n",
       "    ToTensor()\n",
       "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       "    <timm.data.random_erasing.RandomErasing object at 0x13558cd68>\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_train=True\n",
    "from timm.data import create_transform\n",
    "transform = build_transform(is_train, args)\n",
    "transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc4d04a-cfe8-4cdc-9006-e2bb4395c531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ImageNet/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, torch.Tensor, torch.Size([3, 224, 224]), 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root = os.path.join(args.data_path, 'train' if is_train else 'val')\n",
    "print(root)\n",
    "from torchvision import datasets, transforms\n",
    "dataset = datasets.ImageFolder(root, transform=transform)\n",
    "nb_classes = 1000\n",
    "len(dataset),type(dataset[0][0]),dataset[0][0].shape,dataset[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35997edc-f76e-42b1-9bfd-dcd51e946f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(is_train, args):\n",
    "    transform = build_transform(is_train, args)\n",
    "\n",
    "    if args.data_set == 'CIFAR':\n",
    "        dataset = datasets.CIFAR100(args.data_path, train=is_train, transform=transform)\n",
    "        nb_classes = 100\n",
    "    elif args.data_set == 'IMNET':\n",
    "        root = os.path.join(args.data_path, 'train' if is_train else 'val')\n",
    "        dataset = datasets.ImageFolder(root, transform=transform)\n",
    "        nb_classes = 1000\n",
    "    elif args.data_set == 'INAT':\n",
    "        dataset = INatDataset(args.data_path, train=is_train, year=2018,\n",
    "                              category=args.inat_category, transform=transform)\n",
    "        nb_classes = dataset.nb_classes\n",
    "    elif args.data_set == 'INAT19':\n",
    "        dataset = INatDataset(args.data_path, train=is_train, year=2019,\n",
    "                              category=args.inat_category, transform=transform)\n",
    "        nb_classes = dataset.nb_classes\n",
    "\n",
    "    return dataset, nb_classes\n",
    "\n",
    "dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c0a746-be94-4297-a1c2-b7d6dfc9976a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.nb_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a727299-0f49-44a4-8464-55f1fcd788f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 4\n",
       "    Root location: ./data/ImageNet/val\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=256, interpolation=bicubic)\n",
       "               CenterCrop(size=(224, 224))\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "           )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val, _ = build_dataset(is_train=False, args=args)\n",
    "\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7260b442-8905-4cee-8241-775ef1b3a522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.RandomSampler at 0x1358b05f8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1d26865-267b-411b-8383-53203e510431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1358b0208>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    sampler=sampler_train,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    drop_last=True,)\n",
    "\n",
    "data_loader_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6934a81-4f45-4f67-82b3-0bef5428a123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1358b0a20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, \n",
    "    batch_size=int(1.5 * args.batch_size), \n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    drop_last=False)\n",
    "\n",
    "data_loader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cfe86a3-fdae-427c-a20a-86b8c5c88380",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22747038-79ea-4c21-a60b-e66988acdcc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm.models.registry import register_model\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def _cfg_coat(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" Feed-forward network (FFN, a.k.a. MLP) class. \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class FactorAtt_ConvRelPosEnc(nn.Module):\n",
    "    \"\"\" Factorized attention with convolutional relative position encoding class. \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., shared_crpe=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)                                           # Note: attn_drop is actually not used.\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        # Shared convolutional relative position encoding.\n",
    "        self.crpe = shared_crpe\n",
    "\n",
    "    def forward(self, x, size):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # Generate Q, K, V.\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # Shape: [3, B, h, N, Ch].\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]                                                 # Shape: [B, h, N, Ch].\n",
    "\n",
    "        # Factorized attention.\n",
    "        k_softmax = k.softmax(dim=2)                                                     # Softmax on dim N.\n",
    "        k_softmax_T_dot_v = einsum('b h n k, b h n v -> b h k v', k_softmax, v)          # Shape: [B, h, Ch, Ch].\n",
    "        factor_att        = einsum('b h n k, b h k v -> b h n v', q, k_softmax_T_dot_v)  # Shape: [B, h, N, Ch].\n",
    "\n",
    "        # Convolutional relative position encoding.\n",
    "        crpe = self.crpe(q, v, size=size)                                                # Shape: [B, h, N, Ch].\n",
    "\n",
    "        # Merge and reshape.\n",
    "        x = self.scale * factor_att + crpe\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)                                           # Shape: [B, h, N, Ch] -> [B, N, h, Ch] -> [B, N, C].\n",
    "\n",
    "        # Output projection.\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x                                                                         # Shape: [B, N, C].\n",
    "\n",
    "class SerialBlock(nn.Module):\n",
    "    \"\"\" Serial block class.\n",
    "        Note: In this implementation, each serial block only contains a conv-attention and a FFN (MLP) module. \"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 shared_cpe=None, shared_crpe=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv-Attention.\n",
    "        self.cpe = shared_cpe\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.factoratt_crpe = FactorAtt_ConvRelPosEnc(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, \n",
    "            shared_crpe=shared_crpe)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        # MLP.\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, size):\n",
    "        # Conv-Attention.\n",
    "        x = self.cpe(x, size)                  # Apply convolutional position encoding.\n",
    "        cur = self.norm1(x)\n",
    "        cur = self.factoratt_crpe(cur, size)   # Apply factorized attention and convolutional relative position encoding.\n",
    "        x = x + self.drop_path(cur) \n",
    "\n",
    "        # MLP. \n",
    "        cur = self.norm2(x)\n",
    "        cur = self.mlp(cur)\n",
    "        x = x + self.drop_path(cur)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ConvRelPosEnc(nn.Module):\n",
    "    \"\"\" Convolutional relative position encoding. \"\"\"\n",
    "    def __init__(self, Ch, h, window):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "            Ch: Channels per head.\n",
    "            h: Number of heads.\n",
    "            window: Window size(s) in convolutional relative positional encoding. It can have two forms:\n",
    "                    1. An integer of window size, which assigns all attention heads with the same window size in ConvRelPosEnc.\n",
    "                    2. A dict mapping window size to #attention head splits (e.g. {window size 1: #attention head split 1, window size 2: #attention head split 2})\n",
    "                       It will apply different window size to the attention head splits.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(window, int):\n",
    "            window = {window: h}                                                         # Set the same window size for all attention heads.\n",
    "            self.window = window\n",
    "        elif isinstance(window, dict):\n",
    "            self.window = window\n",
    "        else:\n",
    "            raise ValueError()            \n",
    "        \n",
    "        self.conv_list = nn.ModuleList()\n",
    "        self.head_splits = []\n",
    "        for cur_window, cur_head_split in window.items():\n",
    "            dilation = 1                                                                 # Use dilation=1 at default.\n",
    "            padding_size = (cur_window + (cur_window - 1) * (dilation - 1)) // 2         # Determine padding size. Ref: https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338\n",
    "            cur_conv = nn.Conv2d(cur_head_split*Ch, cur_head_split*Ch,\n",
    "                kernel_size=(cur_window, cur_window), \n",
    "                padding=(padding_size, padding_size),\n",
    "                dilation=(dilation, dilation),                          \n",
    "                groups=cur_head_split*Ch,\n",
    "            )\n",
    "            self.conv_list.append(cur_conv)\n",
    "            self.head_splits.append(cur_head_split)\n",
    "        self.channel_splits = [x*Ch for x in self.head_splits]\n",
    "\n",
    "    def forward(self, q, v, size):\n",
    "        B, h, N, Ch = q.shape\n",
    "        H, W = size\n",
    "        assert N == 1 + H * W\n",
    "\n",
    "        # Convolutional relative position encoding.\n",
    "        q_img = q[:,:,1:,:]                                                              # Shape: [B, h, H*W, Ch].\n",
    "        v_img = v[:,:,1:,:]                                                              # Shape: [B, h, H*W, Ch].\n",
    "        \n",
    "        v_img = rearrange(v_img, 'B h (H W) Ch -> B (h Ch) H W', H=H, W=W)               # Shape: [B, h, H*W, Ch] -> [B, h*Ch, H, W].\n",
    "        v_img_list = torch.split(v_img, self.channel_splits, dim=1)                      # Split according to channels.\n",
    "        conv_v_img_list = [conv(x) for conv, x in zip(self.conv_list, v_img_list)]\n",
    "        conv_v_img = torch.cat(conv_v_img_list, dim=1)\n",
    "        conv_v_img = rearrange(conv_v_img, 'B (h Ch) H W -> B h (H W) Ch', h=h)          # Shape: [B, h*Ch, H, W] -> [B, h, H*W, Ch].\n",
    "\n",
    "        EV_hat_img = q_img * conv_v_img\n",
    "        zero = torch.zeros((B, h, 1, Ch), dtype=q.dtype, layout=q.layout, device=q.device)\n",
    "        EV_hat = torch.cat((zero, EV_hat_img), dim=2)                                # Shape: [B, h, N, Ch].\n",
    "\n",
    "        return EV_hat\n",
    "\n",
    "\n",
    "\n",
    "class ConvPosEnc(nn.Module):\n",
    "    \"\"\" Convolutional Position Encoding. \n",
    "        Note: This module is similar to the conditional position encoding in CPVT.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, k=3):\n",
    "        super(ConvPosEnc, self).__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim, k, 1, k//2, groups=dim)\n",
    "\n",
    "    def forward(self, x, size):\n",
    "        B, N, C = x.shape\n",
    "        H, W = size\n",
    "        assert N == 1 + H * W\n",
    "\n",
    "        # Extract CLS token and image tokens.\n",
    "        cls_token, img_tokens = x[:, :1], x[:, 1:]                                       # Shape: [B, 1, C], [B, H*W, C].\n",
    "\n",
    "        # Depthwise convolution.\n",
    "        feat = img_tokens.transpose(1, 2).view(B, C, H, W)\n",
    "        x = self.proj(feat) + feat\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Combine with CLS token.\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding \"\"\"\n",
    "    def __init__(self, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        out_H, out_W = H // self.patch_size[0], W // self.patch_size[1]\n",
    "\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        out = self.norm(x)\n",
    "\n",
    "        return out, (out_H, out_W)\n",
    "\n",
    "class CoaT(nn.Module):\n",
    "    \"\"\" CoaT class. \"\"\"\n",
    "    def __init__(self, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[0, 0, 0, 0], \n",
    "                 serial_depths=[0, 0, 0, 0], parallel_depth=0,\n",
    "                 num_heads=0, mlp_ratios=[0, 0, 0, 0], qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "                 return_interm_layers=False, out_features=None, crpe_window={3:2, 5:3, 7:3},\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.return_interm_layers = return_interm_layers\n",
    "        self.out_features = out_features\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Patch embeddings.\n",
    "        self.patch_embed1 = PatchEmbed(patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dims[0])\n",
    "        self.patch_embed2 = PatchEmbed(patch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n",
    "        self.patch_embed3 = PatchEmbed(patch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n",
    "        self.patch_embed4 = PatchEmbed(patch_size=2, in_chans=embed_dims[2], embed_dim=embed_dims[3])\n",
    "\n",
    "        # Class tokens.\n",
    "        self.cls_token1 = nn.Parameter(torch.zeros(1, 1, embed_dims[0]))\n",
    "        self.cls_token2 = nn.Parameter(torch.zeros(1, 1, embed_dims[1]))\n",
    "        self.cls_token3 = nn.Parameter(torch.zeros(1, 1, embed_dims[2]))\n",
    "        self.cls_token4 = nn.Parameter(torch.zeros(1, 1, embed_dims[3]))\n",
    "\n",
    "        # Convolutional position encodings.\n",
    "        self.cpe1 = ConvPosEnc(dim=embed_dims[0], k=3)\n",
    "        self.cpe2 = ConvPosEnc(dim=embed_dims[1], k=3)\n",
    "        self.cpe3 = ConvPosEnc(dim=embed_dims[2], k=3)\n",
    "        self.cpe4 = ConvPosEnc(dim=embed_dims[3], k=3)\n",
    "\n",
    "        # Convolutional relative position encodings.\n",
    "        self.crpe1 = ConvRelPosEnc(Ch=embed_dims[0] // num_heads, h=num_heads, window=crpe_window)\n",
    "        self.crpe2 = ConvRelPosEnc(Ch=embed_dims[1] // num_heads, h=num_heads, window=crpe_window)\n",
    "        self.crpe3 = ConvRelPosEnc(Ch=embed_dims[2] // num_heads, h=num_heads, window=crpe_window)\n",
    "        self.crpe4 = ConvRelPosEnc(Ch=embed_dims[3] // num_heads, h=num_heads, window=crpe_window)\n",
    "\n",
    "        # Enable stochastic depth.\n",
    "        dpr = drop_path_rate\n",
    "        \n",
    "        # Serial blocks 1.\n",
    "        self.serial_blocks1 = nn.ModuleList([\n",
    "            SerialBlock(\n",
    "                dim=embed_dims[0], num_heads=num_heads, mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n",
    "                shared_cpe=self.cpe1, shared_crpe=self.crpe1\n",
    "            )\n",
    "            for _ in range(serial_depths[0])]\n",
    "        )\n",
    "\n",
    "        # Serial blocks 2.\n",
    "        self.serial_blocks2 = nn.ModuleList([\n",
    "            SerialBlock(\n",
    "                dim=embed_dims[1], num_heads=num_heads, mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n",
    "                shared_cpe=self.cpe2, shared_crpe=self.crpe2\n",
    "            )\n",
    "            for _ in range(serial_depths[1])]\n",
    "        )\n",
    "\n",
    "        # Serial blocks 3.\n",
    "        self.serial_blocks3 = nn.ModuleList([\n",
    "            SerialBlock(\n",
    "                dim=embed_dims[2], num_heads=num_heads, mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n",
    "                shared_cpe=self.cpe3, shared_crpe=self.crpe3\n",
    "            )\n",
    "            for _ in range(serial_depths[2])]\n",
    "        )\n",
    "\n",
    "        # Serial blocks 4.\n",
    "        self.serial_blocks4 = nn.ModuleList([\n",
    "            SerialBlock(\n",
    "                dim=embed_dims[3], num_heads=num_heads, mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n",
    "                shared_cpe=self.cpe4, shared_crpe=self.crpe4\n",
    "            )\n",
    "            for _ in range(serial_depths[3])]\n",
    "        )\n",
    "\n",
    "        # Parallel blocks.\n",
    "        self.parallel_depth = parallel_depth\n",
    "        if self.parallel_depth > 0:\n",
    "            self.parallel_blocks = nn.ModuleList([\n",
    "                ParallelBlock(\n",
    "                    dims=embed_dims, num_heads=num_heads, mlp_ratios=mlp_ratios, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n",
    "                    shared_cpes=[self.cpe1, self.cpe2, self.cpe3, self.cpe4],\n",
    "                    shared_crpes=[self.crpe1, self.crpe2, self.crpe3, self.crpe4]\n",
    "                )\n",
    "                for _ in range(parallel_depth)]\n",
    "            )\n",
    "\n",
    "        # Classification head(s).\n",
    "        if not self.return_interm_layers:\n",
    "            self.norm1 = norm_layer(embed_dims[0])\n",
    "            self.norm2 = norm_layer(embed_dims[1])\n",
    "            self.norm3 = norm_layer(embed_dims[2])\n",
    "            self.norm4 = norm_layer(embed_dims[3])\n",
    "\n",
    "            if self.parallel_depth > 0:                                  # CoaT series: Aggregate features of last three scales for classification.\n",
    "                assert embed_dims[1] == embed_dims[2] == embed_dims[3]\n",
    "                self.aggregate = torch.nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1)\n",
    "                self.head = nn.Linear(embed_dims[3], num_classes)\n",
    "            else:\n",
    "                self.head = nn.Linear(embed_dims[3], num_classes)        # CoaT-Lite series: Use feature of last scale for classification.\n",
    "\n",
    "        # Initialize weights.\n",
    "        trunc_normal_(self.cls_token1, std=.02)\n",
    "        trunc_normal_(self.cls_token2, std=.02)\n",
    "        trunc_normal_(self.cls_token3, std=.02)\n",
    "        trunc_normal_(self.cls_token4, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token1', 'cls_token2', 'cls_token3', 'cls_token4'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def insert_cls(self, x, cls_token):\n",
    "        \"\"\" Insert CLS token. \"\"\"\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def remove_cls(self, x):\n",
    "        \"\"\" Remove CLS token. \"\"\"\n",
    "        return x[:, 1:, :]\n",
    "\n",
    "    def forward_features(self, x0):\n",
    "        B = x0.shape[0]\n",
    "\n",
    "        # Serial blocks 1.\n",
    "        x1, (H1, W1) = self.patch_embed1(x0)\n",
    "        x1 = self.insert_cls(x1, self.cls_token1)\n",
    "        for blk in self.serial_blocks1:\n",
    "            x1 = blk(x1, size=(H1, W1))\n",
    "        x1_nocls = self.remove_cls(x1)\n",
    "        x1_nocls = x1_nocls.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        # Serial blocks 2.\n",
    "        x2, (H2, W2) = self.patch_embed2(x1_nocls)\n",
    "        x2 = self.insert_cls(x2, self.cls_token2)\n",
    "        for blk in self.serial_blocks2:\n",
    "            x2 = blk(x2, size=(H2, W2))\n",
    "        x2_nocls = self.remove_cls(x2)\n",
    "        x2_nocls = x2_nocls.reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # Serial blocks 3.\n",
    "        x3, (H3, W3) = self.patch_embed3(x2_nocls)\n",
    "        x3 = self.insert_cls(x3, self.cls_token3)\n",
    "        for blk in self.serial_blocks3:\n",
    "            x3 = blk(x3, size=(H3, W3))\n",
    "        x3_nocls = self.remove_cls(x3)\n",
    "        x3_nocls = x3_nocls.reshape(B, H3, W3, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # Serial blocks 4.\n",
    "        x4, (H4, W4) = self.patch_embed4(x3_nocls)\n",
    "        x4 = self.insert_cls(x4, self.cls_token4)\n",
    "        for blk in self.serial_blocks4:\n",
    "            x4 = blk(x4, size=(H4, W4))\n",
    "        x4_nocls = self.remove_cls(x4)\n",
    "        x4_nocls = x4_nocls.reshape(B, H4, W4, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # Only serial blocks: Early return.\n",
    "        if self.parallel_depth == 0:\n",
    "            if self.return_interm_layers:   # Return intermediate features for down-stream tasks (e.g. Deformable DETR and Detectron2).\n",
    "                feat_out = {}   \n",
    "                if 'x1_nocls' in self.out_features:\n",
    "                    feat_out['x1_nocls'] = x1_nocls\n",
    "                if 'x2_nocls' in self.out_features:\n",
    "                    feat_out['x2_nocls'] = x2_nocls\n",
    "                if 'x3_nocls' in self.out_features:\n",
    "                    feat_out['x3_nocls'] = x3_nocls\n",
    "                if 'x4_nocls' in self.out_features:\n",
    "                    feat_out['x4_nocls'] = x4_nocls\n",
    "                return feat_out\n",
    "            else:                           # Return features for classification.\n",
    "                x4 = self.norm4(x4)\n",
    "                x4_cls = x4[:, 0]\n",
    "                return x4_cls\n",
    "\n",
    "        # Parallel blocks.\n",
    "        for blk in self.parallel_blocks:\n",
    "            x1, x2, x3, x4 = blk(x1, x2, x3, x4, sizes=[(H1, W1), (H2, W2), (H3, W3), (H4, W4)])\n",
    "\n",
    "        if self.return_interm_layers:       # Return intermediate features for down-stream tasks (e.g. Deformable DETR and Detectron2).\n",
    "            feat_out = {}   \n",
    "            if 'x1_nocls' in self.out_features:\n",
    "                x1_nocls = self.remove_cls(x1)\n",
    "                x1_nocls = x1_nocls.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n",
    "                feat_out['x1_nocls'] = x1_nocls\n",
    "            if 'x2_nocls' in self.out_features:\n",
    "                x2_nocls = self.remove_cls(x2)\n",
    "                x2_nocls = x2_nocls.reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n",
    "                feat_out['x2_nocls'] = x2_nocls\n",
    "            if 'x3_nocls' in self.out_features:\n",
    "                x3_nocls = self.remove_cls(x3)\n",
    "                x3_nocls = x3_nocls.reshape(B, H3, W3, -1).permute(0, 3, 1, 2).contiguous()\n",
    "                feat_out['x3_nocls'] = x3_nocls\n",
    "            if 'x4_nocls' in self.out_features:\n",
    "                x4_nocls = self.remove_cls(x4)\n",
    "                x4_nocls = x4_nocls.reshape(B, H4, W4, -1).permute(0, 3, 1, 2).contiguous()\n",
    "                feat_out['x4_nocls'] = x4_nocls\n",
    "            return feat_out\n",
    "        else:\n",
    "            x2 = self.norm2(x2)\n",
    "            x3 = self.norm3(x3)\n",
    "            x4 = self.norm4(x4)\n",
    "            x2_cls = x2[:, :1]              # Shape: [B, 1, C].\n",
    "            x3_cls = x3[:, :1]\n",
    "            x4_cls = x4[:, :1]\n",
    "            merged_cls = torch.cat((x2_cls, x3_cls, x4_cls), dim=1)       # Shape: [B, 3, C].\n",
    "            merged_cls = self.aggregate(merged_cls).squeeze(dim=1)        # Shape: [B, C].\n",
    "            return merged_cls\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.return_interm_layers:       # Return intermediate features (for down-stream tasks).\n",
    "            return self.forward_features(x)\n",
    "        else:                               # Return features for classification.\n",
    "            x = self.forward_features(x) \n",
    "            x = self.head(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coat_lite_tiny(**kwargs):\n",
    "    model = CoaT(patch_size=4, embed_dims=[64, 128, 256, 320], serial_depths=[2, 2, 2, 2], parallel_depth=0, num_heads=8, mlp_ratios=[8, 8, 4, 4], **kwargs)\n",
    "    model.default_cfg = _cfg_coat()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52295d63-082b-4b5e-b381-90509b65fb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm.models import create_model\n",
    "model = create_model(\n",
    "    args.model,\n",
    "    pretrained=False,\n",
    "    num_classes=args.nb_classes,\n",
    "    drop_rate=args.drop,\n",
    "    drop_path_rate=args.drop_path,\n",
    "    drop_block_rate=args.drop_block,\n",
    "    **eval(args.model_kwargs))\n",
    "\n",
    "model.to(device) \n",
    "model_without_ddp = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe6a1088-3c58-4415-8606-385d3c5cf085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params: 5722856\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f548bf12-af97-40f7-b9f7-2921ad135f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.953125e-06"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_scaled_lr = args.lr * args.batch_size * 1 / 512.0\n",
    "linear_scaled_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcff7233-1ffe-4ac4-8cf6-b61261101f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = linear_scaled_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "edb85665-1588-46c8-95a0-8b5691d715f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 1.953125e-06\n",
       "    weight_decay: 0.0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 1.953125e-06\n",
       "    weight_decay: 0.05\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm.optim import create_optimizer\n",
    "args.opt = 'adamw'\n",
    "args.weight_decay = 0.05\n",
    "optimizer = create_optimizer(args, model)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0f1da3b-d53f-42c2-b1d4-88829deabf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<timm.scheduler.cosine_lr.CosineLRScheduler at 0x13b577668>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm.utils import NativeScaler, get_state_dict, ModelEma\n",
    "from timm.scheduler import create_scheduler\n",
    "\n",
    "args.epochs = 2\n",
    "args.sched = 'cosine'\n",
    "args.min_lr = 1e-5\n",
    "args.decay_rate = 0.1\n",
    "args.warmup_lr = 1e-6\n",
    "args.warmup_epochs = 5\n",
    "args.cooldown_epochs = 10\n",
    "loss_scaler = NativeScaler()\n",
    "lr_scheduler, _ = create_scheduler(args, optimizer)\n",
    "lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "00a7b510-c85a-4f75-bc52-43929e289031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelSmoothingCrossEntropy()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "\n",
    "args.smoothing = 0.1\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7f002392-472d-4515-885d-5ab731ff184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch: [0]  [0/2]  eta: 0:00:02  lr: 0.000001  loss: 6.8070 (6.8070)  time: 1.0231  data: 0.0211\n",
      "Epoch: [0]  [1/2]  eta: 0:00:00  lr: 0.000001  loss: 6.8013 (6.8042)  time: 0.9929  data: 0.0182\n",
      "Epoch: [0] Total time: 0:00:01 (0.9935 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 6.8013 (6.8042)\n",
      "Epoch: [1]  [0/2]  eta: 0:00:01  lr: 0.000001  loss: 7.0222 (7.0222)  time: 0.9399  data: 0.0139\n",
      "Epoch: [1]  [1/2]  eta: 0:00:00  lr: 0.000001  loss: 7.0222 (7.0477)  time: 0.9425  data: 0.0159\n",
      "Epoch: [1] Total time: 0:00:01 (0.9431 s / it)\n",
      "Averaged stats: lr: 0.000001  loss: 7.0222 (7.0477)\n",
      "Test:  [0/2]  eta: 0:00:00  loss: 7.0977 (7.0977)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.4400  data: 0.0284\n",
      "Test:  [1/2]  eta: 0:00:00  loss: 6.9418 (7.0197)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2950  data: 0.0188\n",
      "Test: Total time: 0:00:00 (0.2959 s / it)\n",
      "* Acc@1 0.000 Acc@5 0.000 loss 7.020\n",
      "Accuracy of the network on the 4 test images: 0.0%\n",
      "Max accuracy: 0.00%\n",
      "Training time 0:00:04\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import Iterable, Optional\n",
    "from timm.data import Mixup\n",
    "from collections import defaultdict, deque\n",
    "import math\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not False:\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = [\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}'\n",
    "        ]\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg.append('max mem: {memory:.0f}')\n",
    "        log_msg = self.delimiter.join(log_msg)\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "        \n",
    "        \n",
    "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, loss_scaler, max_norm: float = 0,\n",
    "                    model_ema: Optional[ModelEma] = None, mixup_fn: Optional[Mixup] = None,\n",
    "                    disable_amp: bool = False):\n",
    "    # TODO fix this for finetuning\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 10\n",
    "\n",
    "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        samples = samples.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        if mixup_fn is not None:\n",
    "            samples, targets = mixup_fn(samples, targets)\n",
    "\n",
    "        if disable_amp:\n",
    "            # Disable AMP and try to solve the NaN issue. \n",
    "            # Ref: https://github.com/facebookresearch/deit/issues/29\n",
    "            outputs = model(samples)\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(samples)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if disable_amp:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            # this attribute is added by timm on one optimizer (adahessian)\n",
    "            is_second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n",
    "            loss_scaler(loss, optimizer, clip_grad=max_norm,\n",
    "                        parameters=model.parameters(), create_graph=is_second_order)\n",
    "\n",
    "        # torch.cuda.synchronize()\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "from timm.utils import accuracy, ModelEma\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(data_loader, model, device, disable_amp):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for images, target in metric_logger.log_every(data_loader, 10, header):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        if disable_amp:\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "        else:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(images)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "\n",
    "    print('* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n",
    "          .format(top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))\n",
    "\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "max_accuracy = 0.0\n",
    "args.output_dir = 'output/coat_lite_tiny'\n",
    "args.start_epoch = 0\n",
    "args.clip_grad = None\n",
    "model_ema = None\n",
    "args.disable_amp = True\n",
    "args.save_freq = 2\n",
    "\n",
    "output_dir = Path(args.output_dir)\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    train_stats = train_one_epoch(model, criterion, data_loader_train, optimizer, device, epoch, loss_scaler,\n",
    "                                        args.clip_grad, model_ema, mixup_fn, disable_amp=args.disable_amp)\n",
    "    lr_scheduler.step(epoch)\n",
    "    checkpoint_paths = [output_dir / 'checkpoints/checkpoint.pth']\n",
    "    if epoch % args.save_freq == args.save_freq - 1:\n",
    "        checkpoint_paths.append(output_dir / f'checkpoints/checkpoint{epoch:04}.pth')\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        torch.save({\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'model_ema': None,\n",
    "            'args': args,\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "    if epoch % args.save_freq == args.save_freq - 1:\n",
    "            test_stats = evaluate(data_loader_val, model, device, disable_amp=args.disable_amp)\n",
    "            print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
    "            max_accuracy = max(max_accuracy, test_stats[\"acc1\"])\n",
    "            print(f'Max accuracy: {max_accuracy:.2f}%')\n",
    "\n",
    "            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()}, \n",
    "                         **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                         'epoch': epoch, \n",
    "                         'n_parameters': n_parameters}\n",
    "    else:\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()}, \n",
    "                     'epoch': epoch, \n",
    "                     'n_parameters': n_parameters}\n",
    "        \n",
    "    if args.output_dir:\n",
    "        import json\n",
    "        with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e0d458ff-a67f-4282-a45e-b39adf6e8e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/MAG_Bert_ULGM\n",
      "total 146M\n",
      "-rw-r--r-- 1 mac staff 66M 10 30 21:30 checkpoint.pth\n",
      "-rw-r--r-- 1 mac staff 66M 10 30 21:30 checkpoint0001.pth\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!mkdir -p output/coat_lite_tiny/checkpoints/\n",
    "!ls output/coat_lite_tiny/checkpoints/ -lhrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e280c6a-3c7c-409c-b56d-a51f23d6b69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
