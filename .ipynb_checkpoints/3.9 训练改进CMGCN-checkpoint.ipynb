{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442b0294-3329-4c41-aca8-075065ba6bf5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6836e7-2c20-4981-97b3-9cb0a57b0bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio_feature', 'video_features_p', 'bert_indices', 'box_pad_indices', 'big_graphs', 'labels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class MUStartDataset(Dataset):\n",
    "    def __init__(self,mode = 'train',feature_path = './featuresIndepResnet152.pkl'):\n",
    "        with open(feature_path,'rb') as f:\n",
    "            import pickle\n",
    "            data = pickle.load(f)\n",
    "        self.feature_dict = data[mode]\n",
    "        # [-1,1] -> [0,1]\n",
    "        self.feature_dict['labels'] = ((self.feature_dict['labels'] + 1)/2).astype(np.int64)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        feature ={}\n",
    "        feature['audio_feature'] = self.feature_dict['audio_feature'][index]\n",
    "        feature['video_features_p'] = self.feature_dict['video_features_p'][index]\n",
    "        feature['bert_indices'] = self.feature_dict['bert_indices'][index]\n",
    "        feature['box_pad_indices'] = self.feature_dict['box_pad_indices'][index]\n",
    "        feature['big_graphs'] = self.feature_dict['big_graphs'][index]\n",
    "        feature['labels'] = self.feature_dict['labels'][index]\n",
    "        \n",
    "        return feature\n",
    "    def __len__(self):\n",
    "        labels = self.feature_dict['labels']\n",
    "        length = labels.shape[0]\n",
    "        return length\n",
    "    \n",
    "    def get_sample_shape(self,index):\n",
    "        shape_dict = {}\n",
    "        shape_dict['audio_feature'] = self.feature_dict['audio_feature'][index].shape\n",
    "        shape_dict['video_features_p'] = self.feature_dict['video_features_p'][index].shape\n",
    "        shape_dict['bert_indices'] = self.feature_dict['bert_indices'][index].shape\n",
    "        shape_dict['box_pad_indices'] = self.feature_dict['box_pad_indices'][index].shape\n",
    "        shape_dict['big_graphs'] = self.feature_dict['big_graphs'][index].shape\n",
    "        # shape_dict['labels'] = self.feature_dict['labels'][index].shape\n",
    "        shape_dict['labels'] = type(self.feature_dict['labels'][index])\n",
    "        return shape_dict\n",
    "        \n",
    "d = MUStartDataset('valid')\n",
    "dl = DataLoader(d, batch_size=2, num_workers=0, shuffle=False)\n",
    "batch_sample = iter(dl).next()\n",
    "print(batch_sample.keys())\n",
    "batch_sample['audio_feature'].size(2)\n",
    "batch_sample['video_features_p'].size(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023624a-a495-4f82-b720-bd2494bb7e7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 准备模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2753d11e-1305-4795-8a26-a4359930b1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create CMGCN model\n",
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-uncased/ were not used when initializing MAG_BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing MAG_BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MAG_BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MAG_BertModel were not initialized from the model checkpoint at ./bert-base-uncased/ and are newly initialized: ['bert.MAG.W_hv.weight', 'bert.MAG.W_v.bias', 'bert.MAG.W_a.bias', 'bert.MAG.LayerNorm.bias', 'bert.MAG.W_a.weight', 'bert.MAG.W_ha.weight', 'bert.MAG.W_ha.bias', 'bert.MAG.LayerNorm.weight', 'bert.MAG.W_hv.bias', 'bert.MAG.W_v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_params()\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MAG(nn.Module):\n",
    "    def __init__(self, hidden_size, beta_shift, dropout_prob):        \n",
    "        super(MAG, self).__init__()\n",
    "        print(\"Initializing MAG with beta_shift:{} hidden_prob:{}\".format(beta_shift, dropout_prob))\n",
    "\n",
    "        self.W_hv = nn.Linear(VISUAL_DIM + TEXT_DIM, TEXT_DIM)\n",
    "        self.W_ha = nn.Linear(ACOUSTIC_DIM + TEXT_DIM, TEXT_DIM)\n",
    "        self.W_v = nn.Linear(VISUAL_DIM, TEXT_DIM)\n",
    "        self.W_a = nn.Linear(ACOUSTIC_DIM, TEXT_DIM)\n",
    "        self.beta_shift = beta_shift\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, text_embedding, visual, acoustic):\n",
    "        eps = 1e-6\n",
    "        weight_v = F.relu(self.W_hv(torch.cat((visual, text_embedding), dim=-1)))\n",
    "        weight_a = F.relu(self.W_ha(torch.cat((acoustic, text_embedding), dim=-1)))\n",
    "        h_m = weight_v * self.W_v(visual) + weight_a * self.W_a(acoustic)\n",
    "        em_norm = text_embedding.norm(2, dim=-1)\n",
    "        hm_norm = h_m.norm(2, dim=-1)\n",
    "        DEVICE = visual.device\n",
    "        hm_norm_ones = torch.ones(hm_norm.shape, requires_grad=True).to(DEVICE)\n",
    "        hm_norm = torch.where(hm_norm == 0, hm_norm_ones, hm_norm)\n",
    "        thresh_hold = (em_norm / (hm_norm + eps)) * self.beta_shift\n",
    "        ones = torch.ones(thresh_hold.shape, requires_grad=True).to(DEVICE)\n",
    "        alpha = torch.min(thresh_hold, ones)\n",
    "        alpha = alpha.unsqueeze(dim=-1)\n",
    "        acoustic_vis_embedding = alpha * h_m\n",
    "        embedding_output = self.dropout(\n",
    "            self.LayerNorm(acoustic_vis_embedding + text_embedding)\n",
    "        )\n",
    "\n",
    "        return embedding_output\n",
    "\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertEncoder, BertPooler\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class MultimodalConfig(object):\n",
    "    def __init__(self, beta_shift, dropout_prob):\n",
    "        self.beta_shift = beta_shift\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "class MAG_BertModel(BertPreTrainedModel):\n",
    "    def __init__(self, config, multimodal_config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.MAG = MAG(\n",
    "            config.hidden_size,\n",
    "            multimodal_config.beta_shift,\n",
    "            multimodal_config.dropout_prob,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "    self,\n",
    "    input_ids,\n",
    "    visual,\n",
    "    acoustic,\n",
    "    attention_mask=None,\n",
    "    token_type_ids=None,\n",
    "    position_ids=None,\n",
    "    head_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    singleTask = False,\n",
    "    ):\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        fused_embedding = self.MAG(embedding_output, visual, acoustic)\n",
    "        \n",
    "        encoder_outputs = self.encoder(\n",
    "            fused_embedding,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
    "            1:\n",
    "        ]  # add hidden_states and attentions if they are here\n",
    "        # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        return outputs\n",
    "        \n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias :\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias',None)\n",
    "        \n",
    "    def forward(self, text, adj):\n",
    "        hidden = torch.matmul(text,self.weight)\n",
    "        \n",
    "        denom = torch.sum(adj,dim=2,keepdim=True) + 1\n",
    "        output = torch.matmul(adj, hidden.float())/denom\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "\n",
    "        return output\n",
    "\n",
    "import torch.nn as nn\n",
    "class AlignSubNet(nn.Module):\n",
    "    def __init__(self, dst_len):\n",
    "        \"\"\"\n",
    "        mode: the way of aligning avg_pool 这个模型并没有参数\n",
    "        \"\"\"\n",
    "        super(AlignSubNet, self).__init__()\n",
    "        self.dst_len = dst_len\n",
    "\n",
    "    def get_seq_len(self):\n",
    "        return self.dst_len\n",
    "    \n",
    "    def __avg_pool(self, text_x, audio_x, video_x):\n",
    "        def align(x):\n",
    "            raw_seq_len = x.size(1)\n",
    "            if raw_seq_len == self.dst_len:\n",
    "                return x\n",
    "            if raw_seq_len // self.dst_len == raw_seq_len / self.dst_len:\n",
    "                pad_len = 0\n",
    "                pool_size = raw_seq_len // self.dst_len\n",
    "            else:\n",
    "                pad_len = self.dst_len - raw_seq_len % self.dst_len\n",
    "                pool_size = raw_seq_len // self.dst_len + 1\n",
    "            pad_x = x[:, -1, :].unsqueeze(1).expand([x.size(0), pad_len, x.size(-1)])\n",
    "            x = torch.cat([x, pad_x], dim=1).view(x.size(0), pool_size, self.dst_len, -1)\n",
    "            x = x.mean(dim=1)\n",
    "            return x\n",
    "        text_x = align(text_x)\n",
    "        audio_x = align(audio_x)\n",
    "        video_x = align(video_x)\n",
    "        return text_x, audio_x, video_x\n",
    "    \n",
    " \n",
    "    def forward(self, text_x, audio_x, video_x):\n",
    "        if text_x.size(1) == audio_x.size(1) == video_x.size(1):\n",
    "            return text_x, audio_x, video_x\n",
    "        return self.__avg_pool(text_x, audio_x, video_x)\n",
    "    \n",
    "class CMGCN(nn.Module):\n",
    "    def __init__(self, multimodal_config):\n",
    "        super(CMGCN, self).__init__()\n",
    "        print('create CMGCN model')\n",
    "        # self.bert = BertModel.from_pretrained('./bert-base-uncased/')\n",
    "        self.mag_bert = MAG_BertModel.from_pretrained('./bert-base-uncased/',multimodal_config=multimodal_config)\n",
    "        self.text_lstm = DynamicLSTM(768,4,num_layers=1,batch_first=True,bidirectional=True)\n",
    "        self.vit_fc = nn.Linear(768,2*4)\n",
    "        self.gc1 = GraphConvolution(2*4, 2*4)\n",
    "        self.gc2 = GraphConvolution(2*4, 2*4)\n",
    "        self.fc = nn.Linear(2*4,2)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bert_indices = inputs['bert_indices']\n",
    "        graph = inputs['big_graphs']\n",
    "        box_vit = inputs['video_features_p']\n",
    "        bert_text_len = torch.sum(bert_indices != 0, dim = -1)\n",
    "        # 2,24, audio_feature key 2 33 33 , 2,10 768 \n",
    "        visual = box_vit\n",
    "        acoustic = inputs['audio_feature']\n",
    "        self.align_subnet = AlignSubNet(bert_indices.size(1))\n",
    "        bert_indices, acoustic, visual= self.align_subnet(bert_indices,acoustic,visual)\n",
    "        \n",
    "        acoustic = acoustic.float()\n",
    "        \n",
    "        outputs = self.mag_bert(bert_indices, visual, acoustic)\n",
    "        \n",
    "        encoder_layer = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        text_out, (_, _) = self.text_lstm(encoder_layer, bert_text_len)\n",
    "        # 与原始代码不同，这里因为进行了全局的特征填充，导致text_out可能无法达到填充长度，补充为0\n",
    "        if text_out.shape[1] < encoder_layer.shape[1]:\n",
    "            pad = torch.zeros((text_out.shape[0],encoder_layer.shape[1]-text_out.shape[1],text_out.shape[2]))\n",
    "            text_out = torch.cat((text_out,pad),dim=1)\n",
    "\n",
    "        box_vit = box_vit.float()\n",
    "        box_vit = self.vit_fc(box_vit)\n",
    "        features = torch.cat([text_out, box_vit], dim=1)\n",
    "\n",
    "        graph = graph.float()\n",
    "        x = F.relu(self.gc1(features, graph))\n",
    "        x = F.relu(self.gc2(x,graph))\n",
    "        \n",
    "        alpha_mat = torch.matmul(features,x.transpose(1,2))\n",
    "        alpha_mat = alpha_mat.sum(1, keepdim=True)\n",
    "        alpha = F.softmax(alpha_mat, dim = 2)\n",
    "        x = torch.matmul(alpha, x).squeeze(1)\n",
    "        \n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "def init_params():\n",
    "    for child in cmgcn_model.children():\n",
    "        # print(type(child) != BertModel)\n",
    "        if type(child) != MAG_BertModel:\n",
    "            for p in child.parameters():\n",
    "                # print(type(child))\n",
    "                # print(p.shape, p.requires_grad)\n",
    "                if p.requires_grad :\n",
    "                    # print(len(p.shape))\n",
    "                    if len(p.shape) > 1:\n",
    "                        torch.nn.init.xavier_uniform_(p)\n",
    "                        # print(p[0][:2])\n",
    "                    else:\n",
    "                        import math\n",
    "                        stdv = 1.0 / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "                        # print('else', p[:2])\n",
    "    print('init_params()')\n",
    "    \n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0') if use_cuda else torch.device('cpu')\n",
    "\n",
    "beta_shift = 1.0 \n",
    "dropout_prob = 0.5 \n",
    "multimodal_config = MultimodalConfig(\n",
    "    beta_shift=beta_shift, dropout_prob=dropout_prob\n",
    ")\n",
    "\n",
    "\n",
    "d = MUStartDataset('valid')\n",
    "dl = DataLoader(d, batch_size=2, num_workers=0, shuffle=False)\n",
    "batch_sample = iter(dl).next()\n",
    "\n",
    "ACOUSTIC_DIM = batch_sample['audio_feature'].size(2)\n",
    "VISUAL_DIM = batch_sample['video_features_p'].size(2)\n",
    "TEXT_DIM = 768\n",
    "\n",
    "cmgcn_model = CMGCN(multimodal_config = multimodal_config ).to(device)\n",
    "\n",
    "init_params()    \n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':cmgcn_model.mag_bert.parameters(),'lr':2e-5},\n",
    "    {'params':cmgcn_model.text_lstm.parameters(),},\n",
    "    {'params':cmgcn_model.vit_fc.parameters(),},\n",
    "    {'params':cmgcn_model.gc1.parameters(),},\n",
    "    {'params':cmgcn_model.gc2.parameters(),},\n",
    "    {'params':cmgcn_model.fc.parameters(),},\n",
    "],lr=0.001,weight_decay=1e-5)\n",
    "# optimizer = torch.optim.Adam(cmgcn_model.parameters(),lr=1e-3,weight_decay=1e-5)\n",
    "\n",
    "d = MUStartDataset('valid')\n",
    "dl = DataLoader(d, batch_size=2, num_workers=0, shuffle=False)\n",
    "batch = iter(dl).next()\n",
    "batch.keys()\n",
    "inputs ={}\n",
    "for key in batch.keys():\n",
    "    inputs[key] = batch[key].to(device)\n",
    "outputs = cmgcn_model(inputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7464056-2409-49a0-a395-5dd50f980452",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de29f4a-50c5-4960-84cb-c23e3eb08bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train:----------\n",
      "i_epoch: 0\n",
      "here save the model cmgcn_model.pth\n",
      "early stop\n",
      "test_acc: 0.5\n",
      "test_f1: 0.3333333333333333\n",
      "test_precision 0.25\n",
      "test_recall 0.5\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 1\n",
    "cmgcn_model_path = 'cmgcn_model.pth'\n",
    "\n",
    "# def train():\n",
    "print('start train:' + '-'*10)\n",
    "train_dataset = MUStartDataset(mode='train')\n",
    "valid_dataset = MUStartDataset(mode='valid')\n",
    "test_dataset = MUStartDataset(mode='test')\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=2,num_workers=0,shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=2,num_workers=0,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=2,num_workers=0,shuffle=False)\n",
    "\n",
    "def evaluate_acc_f1(data_loader):\n",
    "    n_correct, n_total = 0, 0\n",
    "    targets_all, outputs_all = None, None\n",
    "    cmgcn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i_batch,batch in enumerate(data_loader):\n",
    "            inputs ={}\n",
    "            for key in batch.keys():\n",
    "                inputs[key] = batch[key].to(device)\n",
    "            outputs = cmgcn_model(inputs)\n",
    "            targets = batch['labels'].to(device)\n",
    "            \n",
    "            n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "            n_total += len(outputs)\n",
    "            \n",
    "            if targets_all is None:\n",
    "                targets_all = targets\n",
    "                outputs_all = outputs\n",
    "            else:\n",
    "                targets_all = torch.cat((targets_all,targets), dim=0)\n",
    "                outputs_all = torch.cat((outputs_all,outputs), dim=0)\n",
    "    \n",
    "    # if macro :\n",
    "    from sklearn import metrics\n",
    "    acc = n_correct / n_total\n",
    "    f1 = metrics.f1_score(targets_all.cpu(), torch.argmax(outputs_all,-1).cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "    precision = metrics.precision_score(targets_all.cpu(), torch.argmax(outputs_all,-1).cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "    recall = metrics.recall_score(targets_all.cpu(), torch.argmax(outputs_all,-1).cpu(), labels=[0,1], average='macro', zero_division=0)\n",
    "\n",
    "    return acc,f1,precision,recall\n",
    "\n",
    "max_val_acc , max_val_f1, max_val_epoch, global_step = 0, 0, 0, 0\n",
    "for i_epoch in range(num_epoch):\n",
    "    print('i_epoch:', i_epoch)\n",
    "    n_correct, n_total, loss_total = 0, 0, 0\n",
    "    for i_batch,batch in enumerate(train_dataloader):\n",
    "        global_step += 1\n",
    "        cmgcn_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        inputs ={}\n",
    "        for key in batch.keys():\n",
    "            inputs[key] = batch[key].to(device)\n",
    "        # print(inputs.keys()) \n",
    "        # dict_keys(['audio_feature', 'video_features_p', 'bert_indices', 'box_pad_indices', 'big_graphs', 'labels'])\n",
    "        outputs = cmgcn_model(inputs)\n",
    "        targets = batch['labels'].to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "        n_total += len(outputs)\n",
    "        loss_total += loss.item() * len(outputs)\n",
    "        \n",
    "        train_acc = n_correct / n_total\n",
    "        train_loss = loss_total / n_total\n",
    "        \n",
    "        if global_step % 1 == 0:\n",
    "            val_acc, val_f1, val_precision, val_recall = evaluate_acc_f1(valid_dataloader)\n",
    "            if val_acc >= max_val_acc:\n",
    "                max_val_f1 = val_f1\n",
    "                max_val_acc = val_acc\n",
    "                max_val_epoch = i_epoch\n",
    "                torch.save(cmgcn_model.state_dict(),cmgcn_model_path)\n",
    "                print('here save the model cmgcn_model.pth')\n",
    "        \n",
    "    if i_epoch - max_val_epoch >= 0:\n",
    "        print('early stop')\n",
    "        break\n",
    "        \n",
    "    break\n",
    "cmgcn_model.load_state_dict(torch.load(cmgcn_model_path))\n",
    "test_acc, test_f1,test_precision,test_recall = evaluate_acc_f1(test_dataloader)\n",
    "# test_acc, test_f1,test_precision,test_recall = evaluate_acc_f1(test_data_loader)\n",
    "print('test_acc:', test_acc)\n",
    "print('test_f1:', test_f1)\n",
    "print('test_precision', test_precision)\n",
    "print('test_recall', test_recall)\n",
    "\n",
    "# return 0\n",
    "# train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
