{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf1df53-522a-4ab4-ad9e-6aa93acd7e02",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 探索mag_bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78009ad2-799e-46a1-9d5f-028c1c050dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102],\n",
      "        [  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]])\n",
      "torch.Size([2, 8, 768])\n",
      "torch.Size([2, 8, 2048])\n",
      "torch.Size([2, 8, 33])\n",
      "torch.Size([2, 8, 2816])\n",
      "torch.Size([2, 8, 768])\n",
      "torch.Size([2, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertEncoder, BertPooler\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "configuration = BertConfig()\n",
    "embeddings = BertEmbeddings(configuration)\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer([\"Hello, my dog is cute\", \"Hello, my dog is cute\"], return_tensors=\"pt\")\n",
    "\n",
    "print(inputs['input_ids'])\n",
    "\n",
    "embedding_output = embeddings(inputs['input_ids'])\n",
    "\n",
    "print(embedding_output.shape)\n",
    "\n",
    "visual = torch.randn((2,8,2048))\n",
    "print(visual.shape)\n",
    "\n",
    "acoustic = torch.randn((2,8,33))\n",
    "print(acoustic.shape)\n",
    "\n",
    "text_embedding = embedding_output\n",
    "vt = torch.cat((visual, text_embedding), dim = -1)\n",
    "print(vt.shape)\n",
    "import torch.nn as nn\n",
    "gvt = nn.Linear(2048 + 768, 768)\n",
    "weight_v = gvt(vt)\n",
    "\n",
    "print(weight_v.shape)\n",
    "layer_vt = nn.Linear(2048, 768)\n",
    "h_m = weight_v * layer_vt(visual)\n",
    "print(h_m.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a946c6-d924-41fa-a74e-b84811128e81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n",
      "MAG(\n",
      "  (W_hv): Linear(in_features=2816, out_features=768, bias=True)\n",
      "  (W_ha): Linear(in_features=801, out_features=768, bias=True)\n",
      "  (W_v): Linear(in_features=2048, out_features=768, bias=True)\n",
      "  (W_a): Linear(in_features=33, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "torch.Size([2, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MAG(nn.Module):\n",
    "    def __init__(self, hidden_size, beta_shift, dropout_prob):        \n",
    "        super(MAG, self).__init__()\n",
    "        print(\"Initializing MAG with beta_shift:{} hidden_prob:{}\".format(beta_shift, dropout_prob))\n",
    "\n",
    "        self.W_hv = nn.Linear(VISUAL_DIM + TEXT_DIM, TEXT_DIM)\n",
    "        self.W_ha = nn.Linear(ACOUSTIC_DIM + TEXT_DIM, TEXT_DIM)\n",
    "        self.W_v = nn.Linear(VISUAL_DIM, TEXT_DIM)\n",
    "        self.W_a = nn.Linear(ACOUSTIC_DIM, TEXT_DIM)\n",
    "        self.beta_shift = beta_shift\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, text_embedding, visual, acoustic):\n",
    "        eps = 1e-6\n",
    "        weight_v = F.relu(self.W_hv(torch.cat((visual, text_embedding), dim=-1)))\n",
    "        weight_a = F.relu(self.W_ha(torch.cat((acoustic, text_embedding), dim=-1)))\n",
    "        h_m = weight_v * self.W_v(visual) + weight_a * self.W_a(acoustic)\n",
    "        em_norm = text_embedding.norm(2, dim=-1)\n",
    "        hm_norm = h_m.norm(2, dim=-1)\n",
    "        DEVICE = visual.device\n",
    "        hm_norm_ones = torch.ones(hm_norm.shape, requires_grad=True).to(DEVICE)\n",
    "        hm_norm = torch.where(hm_norm == 0, hm_norm_ones, hm_norm)\n",
    "        thresh_hold = (em_norm / (hm_norm + eps)) * self.beta_shift\n",
    "        ones = torch.ones(thresh_hold.shape, requires_grad=True).to(DEVICE)\n",
    "        alpha = torch.min(thresh_hold, ones)\n",
    "        alpha = alpha.unsqueeze(dim=-1)\n",
    "        acoustic_vis_embedding = alpha * h_m\n",
    "        embedding_output = self.dropout(\n",
    "            self.LayerNorm(acoustic_vis_embedding + text_embedding)\n",
    "        )\n",
    "\n",
    "        return embedding_output\n",
    "\n",
    "beta_shift = 1.0\n",
    "dropout_prob = 0.5 \n",
    "hidden_size = 768\n",
    "ACOUSTIC_DIM = 33\n",
    "VISUAL_DIM = 2048\n",
    "TEXT_DIM = 768\n",
    "mag_model = MAG(hidden_size,beta_shift,dropout_prob)\n",
    "print(mag_model)\n",
    "text_embedding = torch.randn((2,8,TEXT_DIM))\n",
    "visual = torch.randn((2,8,VISUAL_DIM))\n",
    "acoustic = torch.randn((2,8,ACOUSTIC_DIM))\n",
    "out = mag_model(text_embedding, visual, acoustic)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c9b18-4128-4472-a50c-f21149baeaba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# mag_bert模型的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca6f56bc-69f1-4adf-9843-57b89eb2260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-uncased/ were not used when initializing MAG_BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing MAG_BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MAG_BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MAG_BertModel were not initialized from the model checkpoint at ./bert-base-uncased/ and are newly initialized: ['bert.MAG.W_ha.bias', 'bert.MAG.W_hv.weight', 'bert.MAG.W_hv.bias', 'bert.MAG.W_v.bias', 'bert.MAG.LayerNorm.bias', 'bert.MAG.W_v.weight', 'bert.MAG.W_a.bias', 'bert.MAG.W_ha.weight', 'bert.MAG.LayerNorm.weight', 'bert.MAG.W_a.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"./bert-base-uncased/\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102],\n",
      "        [  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]])\n",
      "torch.Size([2, 8, 768])\n",
      "torch.Size([2, 8, 768])\n",
      "torch.Size([2, 8, 768])\n",
      "torch.Size([2, 768])\n",
      "2\n",
      "torch.Size([2, 8, 768]) torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertEncoder, BertPooler\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class MultimodalConfig(object):\n",
    "    def __init__(self, beta_shift, dropout_prob):\n",
    "        self.beta_shift = beta_shift\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "class MAG_BertModel(BertPreTrainedModel):\n",
    "    def __init__(self, config, multimodal_config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.MAG = MAG(\n",
    "            config.hidden_size,\n",
    "            multimodal_config.beta_shift,\n",
    "            multimodal_config.dropout_prob,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "    self,\n",
    "    input_ids,\n",
    "    visual,\n",
    "    acoustic,\n",
    "    attention_mask=None,\n",
    "    token_type_ids=None,\n",
    "    position_ids=None,\n",
    "    head_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    singleTask = False,\n",
    "    ):\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        print(embedding_output.shape)\n",
    "        fused_embedding = self.MAG(embedding_output, visual, acoustic)\n",
    "        print(fused_embedding.shape)\n",
    "        \n",
    "        encoder_outputs = self.encoder(\n",
    "            fused_embedding,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        print(sequence_output.shape)\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        print(pooled_output.shape)\n",
    "        # 单任务提取pooled_output\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
    "            1:\n",
    "        ]  # add hidden_states and attentions if they are here\n",
    "        # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        print(len(outputs))\n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "beta_shift = 1.0 \n",
    "dropout_prob = 0.5 \n",
    "multimodal_config = MultimodalConfig(\n",
    "    beta_shift=beta_shift, dropout_prob=dropout_prob\n",
    ")\n",
    "print(multimodal_config.beta_shift)\n",
    "mag_bertmodel = MAG_BertModel.from_pretrained('./bert-base-uncased/',multimodal_config=multimodal_config)\n",
    "print(mag_bertmodel.config)\n",
    "\n",
    "\n",
    "ACOUSTIC_DIM = 33\n",
    "VISUAL_DIM = 2048\n",
    "TEXT_DIM = 768\n",
    "visual = torch.randn((2,8,VISUAL_DIM))\n",
    "acoustic = torch.randn((2,8,ACOUSTIC_DIM))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer([\"Hello, my dog is cute\", \"Hello, my dog is cute\"], return_tensors=\"pt\")\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "print(input_ids)\n",
    "outputs = mag_bertmodel(input_ids, visual, acoustic)\n",
    "print(outputs[0].shape,outputs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a529b007-7816-43e5-b79a-35f1f767f36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
